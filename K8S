Ref:
- https://www.youtube.com/watch?v=r0uRLhrzbtU&list=PL2We04F3Y_43dAehLMT5GxJhtk3mJtkl5&index=1: Mumshad
- https://www.youtube.com/watch?v=bhBSlnQcq2k&t=10652s : By TechwithNana
- https://www.youtube.com/watch?v=X48VuDVv0do&t=138s By TechwithNana
- https://www.youtube.com/watch?v=YHuZ78Ig_oc&list=PLrMP04WSdCjrkNYSFvFeiHrfpsSVDFMDR : Pavan Elthepu
- https://www.youtube.com/watch?v=DsHcfoRyDsM&list=PLTyWtrsGknYfanKF33E12LdJvl5q5PZGp&index=1: Tech Primers
- https://www.youtube.com/watch?v=J9CqfcKmkEw&list=PLKDtzUTXWwwEVpu7OzRkEf4zlmVBTzgxr&index=1: 
- https://www.youtube.com/watch?v=tsAH_Vv8GOI&list=PLy0Gle4XyvbFkHCDcJEYG2f6q1dVNJ64- :Peter Jausovec

----------------------: INTRO / OVERVIEW OF K8S :---------------------------------
Version:

- Developed by Google labs (2003) later donated to CNCF (Cloud native Computing Foundation) in collab with Linux foundation. 
- Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerised applications. The open source project is hosted by the Cloud Native Computing Foundation (CNCF).
- Smallest unit in K8s is POD.

- Enterprise k8s distribution , build on the top of the k8s with additional funtionality. 
- Any functionality is missing with k8s , they have added.
 - VMWare Tanzu
 - Redhat Openshift --> provides all free trainings. (self placed)
 - SUSE Rancher --> provides all free tranings. (self placed)

- K8s doesn't natively support monitoring, logging & tracing .(vanilla k8s)
- K8s doesnot natively support CICD Pipeline. 

 EKS : Amazon Elastic k8s service
 AKS : Azure K8s service
 GKE : Google k8s Engine
 Digitalocean K8s
 IBM Cloud K8s Services
 Oracle Container Engine for k8s
 VMWare Tanzu K8s Grid

 -->  self Managed: Openshift by Redhat and Docker enterprise. 

IMP TOPICS:
 1) Architechture
 2) Replication Type
 3) Networking
 4) Services
 5) Ingress
 6) Namespace
 7) Helm Charts
 8) 

 ---------------------: INTERVIEW QUS :------------------------
 1) Namespace 
 2) Why we used Namespace
 3) Architechture of K8S  
 4) How to check pod logs in k8s 
 5) how to delete job in k8s
 6) How to check nodes cpu utilisation
 7) How to check node quota
 8) How to assign permisson to particular group
 9) How to search image in Docker
 10) How to kill process of particular users
 11) How to copy data between directory.
 12) Tar command parameters
 13) How to take volume backup
 14) How to deploy k8s

 15) Why not docker swarm instead of K8s??
   - Cluster is not robust.
   - K8s Autoscaling , if pod dies automatically its create a new one
   - rolling update
   - integrate tools for logging & monitoring like promethus & grafana etc. 

---------------------: COMPONENTS / ARCHITECTURE OF K8S : ------------------

https://kubernetes.io/docs/concepts/overview/components/

Master Node / Control Plane (3 Node) - Scheduler , API Server, ETCD, Control manager, Cloud manager , Container runtime , Kubelet, Kubeproxy . 
Worker Node  - Kubelet , Kube-proxy, Container runtime (Docker engine / CRI-O / Containerd), Pod(IP Address, Volume, Docker). 

Ex:
Pod1: App1
Pod2: App2

1. An API Server: kubectl cmds , Heart of K8s
 - Its acts like a front-end / Main access point to the control panel.
 - The users, management devices, CLI interfaces all talk to the API Server to interact with the k8s cluster. 

2. An ETCD Service: ‘D’ distributed system
 - Its a distributed reliable key-value store used by k8s to store all data used to manage the cluster. 
 - When we have multiple nodes and multiple masters in our cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. 
 - Also its responsible for implementating locks within the cluster to ensure there is no conflicts between the Masters.
 - Data is never overwritten , it always append. 
 - ETCD CLI managment tool - etcdcli, provides snapshot save & restore. 
 - It's written in Go Language.
 - Also used to store subnets, Configmaps, secrets etc.

3. Scheduler:
 - Responsible for distributing works / containers across multiple nodes. 
 - It looks for newly created containers & assigns them to nodes. 

4. Controller: 
 - Controllers are the brain behind orchestration. 
 - They are responsible for noticing & responding when nodes, containers or endpoints goes down.
 - The controllers make decision to bring up new containers in such cases.
 - Knows the state of the cluster / node / pods. 

5. Container Runtime : Docker
 - Container runtime is the underlying software that is used to run containers .

6. Kubelet: "Agent"
 - Kubelet is the agent that runs on each node in the cluster.
 - The agent is responsible for making sure that the containers are running on the nodes as expected.
 - Also it is responsible to interacting with the masters to provide health info of the worker node. 
 - Kubelet uses a "shim" application. 
 - Swap disabled. You MUST disable swap in order for the kubelet to work properly.

7. Kube-proxy:
 - It's forward's the request.
 - Responsible for ensuring the network traffic is routed properly to internal & external services as required. 
 - kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.


----- KUBEADM HA Topology - Stacked etcd ---------------------
- Out of 3 etcd, one would be active.
 Ex:
  Worker Node_1 --  Worker_Node_2 --  Worker_Node_3
  -------------  Load Balancer --------------------
  Control_panel_1 -- control_panel_2 --control_panel_3

----- KUBEADM HA Topology - External etcd ---------------------
- etcd running outside the control plane .
- Have to take care of the replication.

----------------------: Installation: -----------------------

---------------------: Kubernetes Definition File :-----------------------------------

Kubernetes definition file always contains four top level fields:
1. "apiVersion": version of the k8s to creating the object ( v1 ,apps/v1 )
2. "kind": type of object : (Pod , Service, ReplicaSet, Deployment )
3. "metatdata": data / information about the object ( names , lables )
       name: could be a string value ( myapp-pod)
       namespace:
       lables: (We can filter the pod )
         name:
 	     app: able to filter out the pods by giving the unique name ( myapp ) 
4. "spec": specification section , Defines whats inside th object we're creating. 
       containers: name of the pod within the container
       	 - name: nginx-container - name of the container i.e nginx container 
       	   image: nginx - docker hub image name to create 
       	   ports:
       	       - containerPort: 80
       	   env:
       	       - name: POSTGRES_USER
       	         value: "******"
       	       - name: POSTGRES_PASSWORD
       	         value: "*****"
       	  
Example 1: pod.yml
apiVersion: v1
kind: Pod
metadata:
 name: nginx
 lables: 
   app: nginx
   tier: frontend
spec:
 containers:
   - name: nginx
     image: nginx 
   - name: busybox
     image: busybox
 #kubectl create / apply -f pod.yml
 #kubectl get pods
 #kubectl describe pod nginx 
 #kubectl delete pod nginx

------------------------------: K8S IMP CMDS: -------------------------------------------------
# kubectl api-resources | more -->how many resources we can manage/create.
 - false: can't be run in ns.
 - true: can be deployed within the namespace.
#kubectl explain pod
#kubectl explain pod.metadata | more
#kubectl explain pod.spec | grep -i require

#kubectl get nodes | po | deployment | sv | rs | ns | pv | pvc, to get the status

#kubectl get pods -o wide , -o:output format, can see the details of  node / IP/ age / status etc..
#kubectl delete pod <podname>, to delete the pod named webapps, 
#kubectl exec -it <podname> -- bash , to enter within a specific pod
#kubectl exec -it <podname> -c <containername> -- sh/bash , When we have multiple container in a pod. 
#kubectl describe pod <ngnix> , details/informations of the single pod.
#kubectl logs <podname> -f , '-f' to watch / monitoring the logs, to see the pod logs
#kubectl get pods -l name=myapp, we can see the pods falls under the lables. "filter the pods"
#kubectl port-forward <podname> 8080:80, localport to the container port. localhost:8080

#kubectl cluster-info - 
#kubectl expose deployments - 
#kubectl run nginx --image=nginx - single command to create nginx pod 
#kubectl create -f pod-definition.yml - '-f' file name - create nginx pod with definition.yml file
#kubectl apply -f pod-definition.yml - create nginx pod with definition.yml file
#kubectl api-resources | grep replicaset - to get the details for replicaset / deployments
#kubectl api-resources | grep services - to get the details of services 
#kubectl get replicationcontroller
#kubectl get replicasets / rs
#kubectl get all
#kubectl describe deployment myapp-deployment
#kubectl get pods,svc - Can see both pods running status aswell as services 
#kubectl scale --replicas=6 -f replicas-definition.yml , 1st we edit the definition file and apply the scale-in /scale-out 
#kubectl get deployment <nginx-deployment> -o yaml > results.yaml

Status of rollout:
 # kubectl rollout status deployment/myapp-deployment  - 
 #kubectl rollout history deployment/myapp-deployment - to see the revision & history of deployments
 #kubectl rollout undo deployment/myapp-deployment - i.e rollback to the previous version of deployment, notice somthing change / not working after that we can use this command

------------------------------------------------------------------------

------------------ Namespace ---------------
------------------ Service -----------------
------------------ Deployment --------------
 ----------------- ReplicaSet --------------
  ---C1--------C2------   ---c1-----c2------
  ---------- Pod-1-----   -----Pod2---------

-------------------: Kubernetes Replication Tyes :--------------------------
 Why :
     - Self Healing, if any pods goes down , bringing back is called self healing.
     - Roll-out & Roll-back, 1.21 --> 1.22 & 1.22 --> 1.21
     - High Availability
Types:
1. ReplicaSet() --> desired count/replicas & actual count matches, any mismatch going to do self healing.
2. Deployment --> Gives us rollout & rollback
3. DeamonSet --> One pod per node.
4. statefulSet --> Maintain a sticky identity for each of their pods.
-------------------------------
   1. ReplicaSet (new, recommended way) , requires a "selector" definition helps to identify what pod falls under it . 
   2. Deployment 
Differences:
     Major difference between replicaset & Deployment :
       1. ReplicaSet - always makes sure desired number of replica's are always available .
       2. Deployment- except kind which is deployment . rollout & rolling updates , Whenever we create the deployment , deployment recreate the replicaset & replicaset create the pods .
    3. DeamonSet --> One pod per node.
    4. statefulSet --> Maintain a sticky identity for each of their pods.
-----------------------: Kubernetes Services: -----------------------------------------------
# kubectl get svc --> "kubernetes" is the default service.
Why:
    - Enables the communication between various components within & outside of the application . 
Types:
     1. Node port ( 30,000 - 32,767)
     2. ClusterIP (Port - 443 / TCP) Default one , Expose's the pod Internal to the cluster , when we don't want to expose the application to the outside world i.e database's 
     3. Load balancer
     4. Multi-Port , For some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object.
Advantages:
      - Load Balancing 
      - Zero downtime 
      - service discovery 
      
-----------------------: Kubernetes Replication Tyes : -----------------------

1. Replication Controller : One fails we still have another one , providing high availability , can help automatically bringing up the new pods even its just one or hundrade  .

Example: rc-definition.yml 
 1. apiVersion: v1
 2. kind: ReplicationController  
 3. metadata: --> for replicationcontroller
    name: myapp-rc
    lables:
  	  app: myapp
  	  type: front-end
 4. spec: --> for replicationcontroller
 	1. template: to provide the pod template. --> For Pod
 		  metadata: 
 			  name: myapp-pod
 			  lables:
 			   app: myapp
 			   type: front-end
 		  spec: --> For Pod (image,pod name, port)
 		    containers:
 		 	   - name: nginx-container
 		 	     image: nginx
           ports:
            - containerPort: 8080
 	2. replicas: 3

  # kubectl create -f rc-definition.yml
  # kubectl get replicationcontroller - to view the list of created replicationcontroller.
  # kubectl get pods
  # kubectl get all
 Note: All are starting with replicationcontroller name i.e., "myapp-rc"
 
2. Replicaset: replicaset-definition.yml
  - "Labels & Selectors:" Monitors the exisiting pods as its already created , if anyone fails then its deploy a new one .
  - "label  " uses as filter section.
  - "selector" to connect to pod through labels.
 	
 Example:
 1. apiVersion: apps/v1
 2. kind: ReplicaSet , "Note: R & S should be in capital"
 3. metadata:
 	   name: myapp-replicaset
 	 lables:
 		- name: myapp
 		  type: front-end
 4. spec:
  	 	1. template:
 		    1. metadata: 
 			    name: myapp-pod
 			    lables:
 				 app: myapp
 				 type: front-end
 		   2. spec: 
 		        containers:
 		 	     - name: nginx-container
 		 	       image: nginx
      2. replicas: 3
 	    3. selector: ----> ( copy the lables from the pod definition file and paste it under the selector section )
 	  	     matchLables: 
 	  		   type: front-end

 Two ways to scale the replicaset :
 	1st: edit to 6 and use cmd #kubectl apply -f replicase-definition.yml --> Recommended Way.
 	2nd: # kubectl scale --replicas=6 -f replicas-definition.yml 
 	
3. Deployments: 
 - Updates & Rollbacks in a Deployments:
     Rollout & Versioning in a deployment:
      - when we 1st create a deployment , it triggers a rollout , a new rollout creates a new deployment revision-1, 
      - In further when application upgraded / updated a new revision-2 created.
      - This helps us keep track changes and enables us to rollback in future if necessary .
      - Whenever we create the deployment , deployment recreate the replicaset & replicaset create the pods .
        
Status of rollout:
 # kubectl rollout status deployment/myapp-deployment  - 
 # kubectl rollout history deployment/myapp-deployment - to see the revision & history of deployments
 # kubectl rollout undo deployment/myapp-deployment - notice somthing change / not working after that we can use.
 - this command i.e rollback to the previous version of deployment

Deployment strategies:
 Two types
 1. recreate strategy , 1st destroy previous version & create / deploy new application versions - downtime
 2. Rolling update , default strategy, one by one upgrade i.e one down & update like this

----------------------------------------------------------------------
Networking in K8S:
 cisco , cilium, flannel , vmware NSX, calico (project calico)

--------------------------: Kubernetes Services: -------------------------------------------
  Enables the communication between various components within & outside of the application . 
 1. Node port ( 30,000 - 32,767) / TCP --> Stable IP Address. 
 2. Cluster IP , default one , Port - 443 / TCP
 3. Load balancer
 
 1. Node Port:
    - Exposes the Service on each Node's IP at a static port (the NodePort). 
    - A ClusterIP Service, to which the NodePort Service routes, is automatically created. 
    - You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>
  Example :
      apiVersion: v1 
      Kind: Service
      Metadata:
        name: myapp-service
      spec: 
        type: NodePort
        ports:
         - targetPort: 80 ---1 (withinpod)
            port: 80 ---2 (service port)
            nodeport: 30008 ---3 (node)
       selector:  ----> "Could be hundrade of pods , so we will use lables & selector to link together " pull the lables from the pod and place under selector section . 
         app: myapp
         type: front-end
# kubectl create -f service-definition.yml
# kubectl get sv - list the service , where we can able to see cluster IP & NodePort
# curl https://192.186.1.2:30008 

2. ClusterIP: 
  - Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.
  - As Pod ip can not be static as it may goes down and again recreate with new one so, we can't relie on this ip for internal communication between the applications .
  # kubectl port-forward service/nginx-service 8083:8082

3. LoadBalancer:
 - Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
 - New load balancer created and we need to pay , When we have multiple services it will increases the cost and difficult to manage. 
 - If we're using bare metal then we've to set entrypoint like proxy-server. 
 
 Microservices Application on K8S:
     1. First we need to create the container by using the images "docker run cmd" to start an instance / container . (all images of the applications are already build and avaialble in dockerhub repo)
     2. deploy pods
     3. enable connectivity between the services . what application required .
     4. Create the services ( ClusterIP & NodePort )

----------------------------: Kubernetes Ingress: ---------------------------------------------
What is Ingress?
  - We can access the application from outside of the cluster. 
  - We declare which request should go to which service.So we should write the ingress rule for this. 
  - For ingress rule we should have deploy "Ingress Controller" pod into our cluster to process the ingress rule.
  - Ingress Controller could be an entrypoint to the cluster.

How it Works ? 
http-->load balancer-->ingress controller-->reads ingress rules-->forwards to service-->pod
 1. User hit the http request i.e http:/www.xyz.com
 2. Forwards the service to load balancer.
 3. Load balancer forward the service to ingress controller pod and reads the ingress rules and decide which service to call.
 4. Once decides which service to call it forwards the request to service. 
 5. And reaches to desired pod.

Imp Note:
- Now we don't need any Nodeport or load balancer services as ingress controller is a pod and can access ClusterIP services. 

Benefits over NodePort / LoadBalancer:
  - We can include our own authentication .

Ingress Controller: Mostly third parites.
Types:
  - "Nginx Ingress Controller" only maintained by K8s itself.
  - HA Proxy
  - traefik
  - Istio etc
# kubectl get po -n ingress-nginx - to verify nginx ingress controller and get expose through service.
# kubectl get svc -n ingress-nginx - Nodeport service is created for nginx ingress controller., if cloud it should create Load balancer service. 
# kubectl describe ing nginx-ingress
 
------------------------: Kubernetes Namespaces: ------------------------------
 1. What is Namespace?
  - Way to organise the cluster into the virtual sub-clusters, so we are creating the resources (pod,svs,rs,deployment,secrets,ingress,configs) in these namespaces instead of creating all in one namespaces. 
  - Each namespace logically separated with each other but can communicate with each other. 
  - When we create k8s cluster 4 default namespaces created ( Default, kube-node-lease, kube-public, kube-system )
       1. Default - By default resource created  within default namespace , Resource when we don't specify namespace explicitly
       2. Kube-node-lease - Contains lease resources to send the heart_beats of node , if nodes goes down , so Pods within that node will be created with healthy node . Lease will take action . 
       3. kube-public - Used for public resource . Open to all users with read only access. 
       4. kube-system - For Objects created by k8s (etcd,apiserver,kubeproxy,scheduler,control manager)
  
 2. Need of Namespace?
   - Avoid conflicts like same name between teams.
   - Restricting Access like Dev & Prod .
   - Resource limits by using resource quota , i.e allocated to the individual application according to their requirement .(CPU,RAM,Storage per NS etc)
  
 3. Namespaces in action?
   Can be created in two way
     #kubectl create namespace <nginx> , here nginx is the name of the namespace -> (1st one) 
     #kubectl get namespaces ->  2nd one - Byusing ConfigMap file ( Better recommendation )
  
  Ex: By using Configuration file.
   apiVersion: v1
   Kind: ConfigMap
   metatdata:
    name: mysql-configmap
    namespace: my-namespace
   spec:
    db_url: <>
   
   spec - is optional in namespace 
   #kubectl get all --name-namspaces or kubectl get all -A
  
==============================: Helm-3 :==========================================================
Version: 2 vs 3
 - Version2 comes in two parts (client,server model)
  - helm client 
  - server (tiller)
  - Don't have role based control access. 

 - Version3 (Client)
  - Single client architecture.
  - Releases in Nov2019.
 When we execute the cmd helm install <chartname> it ->send the request to tiller
  - Helm client & libraries written in Go language. 
---------------------------------
Like yum & apt -->package manager (automated installation, versioning, dependecies mgmt, remove etc)

What is Helm?
 - Introduce in 2015
 - Package manager for k8s.
 - To package YAML files and distributes in public & private repo.(helmhub)
 - Helm automatically maintains a database of all versions of our releases. 

What are Helm charts?
 - Bundle of YAML files.
 - We can create our own helm charts with helm
 - download and can use.
 - With the help of helm charts we can define, install, upgrade the application. 
 Ex:
 Database apps, Mongo DB, Elasticsearch, mySQL, Prometheus etc..

Why we need helm ?
 - Writing & maintaing k8s manifest file can be time consuming & tidious work. 
 - Min three yaml files need. 
 Ex: deployment, secretes, configmaps, services etc
 
Helm chart Structure:
- Directory structure: ls
<Chart.yaml charts templates values.yaml>
  #tree mycharts
  mycharts/ -->folder
  - Charts.yaml -->meta info about chart, name, version, dependecies etc.
  - values.yaml --> place where all the values can be configure/ default value can be configured .
  - charts/ --> folder contain chart dependecies. 
  - templates/ -->folder contains actual templates files stored.
    - .txt
    - _helpers.tpl
    - deployment.yaml
    - ingress.yaml
    - service.yaml
    - serviceaccount.yaml
   ..
 #helm install <chartname>
  NAME:
  LAST DEPLOYED:
  NAMESPACE:
  STATUS:
  REVISION:
 #kubectl get all

Heml Feature:
1. Templating Engine:
 - define a common blueprint
 - Dynamic values replaced by placeholders.

2. Same application across different clusters:
 Ex: dev, QA, staging, prod etc.

3. Release Management:
 - Helm automatically maintains a database of all versions of our releases. 

:Examples:
---> Normal yaml file <---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: Release-springboot
spec:
 replicas: 2
 selector:
  matchlabels: 

---> Helm file <---
- we need to defines values.yaml file there we need to go replicaCount, and place it here. 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: Release-springboot
spec:
 replicas: {{.values.replicaCount}}
 selector:
  matchlabels: 

------Heml Cmds----------
helm list
helm repo list
helm repo add <repo_name> <repo_url>
helm repo remove <repo_name>
helm search repo jenkins
helm show --> info about chart
helm show <values|chart|readme|all> <chart_name>
helm install <release_name> <chart_name>
helm install <release_name> <chart_name> --version 0.1.0
helm create 
helm install --dry-run <release_name> <chart_name>
helm status <release_name>
helm history <release_name>
helm upgrade <release_name> <chart_name> 
helm rollback <release_name>
helm pull <chart_name> - in the form of tar file
helm pull -untar <chart_name>

========================= "Readnessprob" & "Livenessprob / Healthcheck" ====================
- Application Healthcheck.
- In Order to verify whether application is running or not within the container we can use livenessprob. 
- If the cmds succeds, it returns 0 and the kubelet consider the container is alive & healthy. 
- if it's get nonzero then kills the pod/container & recreate it. 
Ex:
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: mylivenessprobe
spec:
  containers:
  - name: liveness
    image: ubuntu
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 1000
    livenessProbe:                                          
      exec:
        command:                                         
        - cat                
        - /tmp/healthy
      initialDelaySeconds: 5          
      periodSeconds: 5                                 
      timeoutSeconds: 30 

  echo $? --> show health condition either 0 or 1

========================= "K8S VOLUME" =====================================
Link: https://www.youtube.com/watch?v=9zjGOCb-6As&list=PLBGx66SQNZ8aPsFDwb79JrS2KQBTIZo10&index=53

How to persists data in k8s using volumes?
1. Persistent Volume (Always Available)
2. Persistent Volume Claim
3. Storage Class

- When Pod gets restarted all the data gets deleted. so need to configure database each app explicitly. 
- Storage must be available in all nodes. 
- Storage needs to survive even if cluster crashes. 

1. Persistent Volume
  - Are not namespaces. 
  - Cluster resource just like RAM CPU Use to store data
  - Created by YAML file
  - kind: PersistentVolume
  - spec: how much storage?
  - needs actual physical storage like local disk, cloud storage, nfs etc

2. Persistent Volume Claim
Pod request the volume through the PV claim, claim tries to find a volume in cluster, Volume has the actual storage backend. 
  - Claims must be in the same namespace.
  - Created by YAML file
  - kind: PersistentVolumeClaim
  - spec: 

3. Storage Class
 
=================================
PERSISTENT VOLUME EXAMPLE
================================
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myebsvol
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  awsElasticBlockStore:
    volumeID:           # YAHAN APNI EBS VOLUME ID DAALO
    fsType: ext4
============
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myebsvolclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pvdeploy
spec:
  replicas: 1
  selector:      # tells the controller which pods to watch/belong to
    matchLabels:
     app: mypv
  template:
    metadata:
      labels:
        app: mypv
    spec:
      containers:
      - name: shell
        image: centos
        command: ["bin/bash", "-c", "sleep 10000"]
        volumeMounts:
        - name: mypd
          mountPath: "/tmp/persistent"
      volumes:
        - name: mypd
          persistentVolumeClaim:
            claimName: myebsvolclaim

 

  
 
 	
 
 



