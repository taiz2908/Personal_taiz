Ref:
- https://www.youtube.com/watch?v=r0uRLhrzbtU&list=PL2We04F3Y_43dAehLMT5GxJhtk3mJtkl5&index=1: Mumshad
- https://www.youtube.com/watch?v=bhBSlnQcq2k&t=10652s : By TechwithNana
- https://www.youtube.com/watch?v=X48VuDVv0do&t=138s By TechwithNana
- https://www.youtube.com/watch?v=YHuZ78Ig_oc&list=PLrMP04WSdCjrkNYSFvFeiHrfpsSVDFMDR : Pavan Elthepu
- https://www.youtube.com/watch?v=DsHcfoRyDsM&list=PLTyWtrsGknYfanKF33E12LdJvl5q5PZGp&index=1: Tech Primers
- https://www.youtube.com/watch?v=J9CqfcKmkEw&list=PLKDtzUTXWwwEVpu7OzRkEf4zlmVBTzgxr&index=1: 
- https://www.youtube.com/watch?v=tsAH_Vv8GOI&list=PLy0Gle4XyvbFkHCDcJEYG2f6q1dVNJ64- :Peter Jausovec
- https://www.youtube.com/watch?v=PN3VqbZqmD8 - Kubernetes 101 workshop - complete hands-on

- https://k8syaml.com/ --> k8s yaml generator.
---------------------------------------------------------------------------------
Projects:
 - Deploy a Reddit Clone with Kubernetes Ingress: https://www.youtube.com/watch?v=9tl0A_rwgu4

----------------------: INTRO / OVERVIEW OF K8S :---------------------------------
Version: 
 v1.26 -->latest
 v1.25 
 v1.24
 v1.23

- Developed by Google labs (2003) later donated to CNCF (Cloud native Computing Foundation) in collab with Linux foundation. 
- Kubernetes is an open source container orchestration engine for automating deployment, scaling, and management of containerised applications. The open source project is hosted by the Cloud Native Computing Foundation (CNCF).
- Smallest unit in K8s is POD.

- Enterprise k8s distribution , build on the top of the k8s with additional funtionality. 
- Any functionality is missing with k8s , they have added.
 - VMWare Tanzu
 - Redhat Openshift --> provides all free trainings. (self placed)
 - SUSE Rancher --> provides all free tranings. (self placed)

- K8s doesn't natively support monitoring, logging & tracing .(vanilla k8s)
- K8s doesnot natively support CICD Pipeline. 

K8s in Public cloud:
 - EKS : Amazon Elastic k8s service
 - AKS : Azure K8s service
 - GKE : Google k8s Engine
 - Digitalocean K8s
 - IBM Cloud K8s Services
 - Oracle Container Engine for k8s
 - VMWare Tanzu K8s Grid

 -->  self Managed: Openshift by Redhat and Docker enterprise. 

IMP TOPICS:
 1) Architechture
 2) Replication Type
 3) Networking
 4) Services
 5) Ingress
 6) Namespace
 7) Helm Charts
 8) 

---------------------: COMPONENTS / ARCHITECTURE OF K8S : ------------------
- https://kubernetes.io/docs/concepts/overview/components/

Master Node / Control Plane (3 Node) - Scheduler , API Server, ETCD, Control manager, Cloud manager , Container runtime , Kubelet, Kubeproxy . 
Worker Node  - Kubelet , Kube-proxy, Container runtime (Docker engine / CRI-O / Containerd), Pod(IP Address, Volume, Docker). 

Ex:
Pod1: App1
Pod2: App2

1. An API Server: kubectl cmds , Heart of K8s
 - Its acts like a front-end / Main access point to the control panel.
 - The users, management devices, CLI interfaces all talk to the API Server to interact with the k8s cluster. 

2. An ETCD Service: ‘D’ distributed system
 - Its a distributed reliable key-value store used by k8s to store all data used to manage the cluster. 
 - When we have multiple nodes and multiple masters in our cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. 
 - Also its responsible for implementating locks within the cluster to ensure there is no conflicts between the Masters.
 - Data is never overwritten , it always append. 
 - ETCD CLI managment tool - etcdcli, provides snapshot save & restore. 
 - It's written in Go Language.
 - Also used to store subnets, Configmaps, secrets etc.

3. Scheduler:
 - Responsible for distributing works / containers across multiple nodes. 
 - It looks for newly created containers & assigns them to nodes. 

4. Controller: 
 - Controllers are the brain behind orchestration. 
 - They are responsible for noticing & responding when nodes, containers or endpoints goes down.
 - The controllers make decision to bring up new containers in such cases.
 - Knows the state of the cluster / node / pods. 

5. Container Runtime : Docker
 - Container runtime is the underlying software that is used to run containers .

6. Kubelet: "Agent"
 - Kubelet is the agent that runs on each node in the cluster.
 - The agent is responsible for making sure that the containers are running on the nodes as expected.
 - Also it is responsible to interacting with the masters to provide health info of the worker node. 
 - Kubelet uses a "shim" application. 
 - Swap disabled. You MUST disable swap in order for the kubelet to work properly.
Note:
The kubelet uses "liveness probes" to know when to restart a container.

7. Kube-proxy:
 - It's forward's the request.
 - Responsible for ensuring the network traffic is routed properly to internal & external services as required. 
 - kube-proxy maintains network rules on nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.


----- KUBEADM HA Topology - Stacked etcd ---------------------
- Out of 3 etcd, one would be active.
 Ex:
  Worker Node_1 --  Worker_Node_2 --  Worker_Node_3
  -------------  Load Balancer --------------------
  Control_panel_1 -- control_panel_2 --control_panel_3

----- KUBEADM HA Topology - External etcd ---------------------
- etcd running outside the control plane .
- Have to take care of the replication.

----------------------: Installation: -----------------------

---------------------: Kubernetes Definition File :-----------------------------------

Kubernetes definition file always contains four top level fields:
1. "apiVersion": version of the k8s to creating the object ( v1 ,apps/v1 )
2. "kind": type of object : (Pod , Service, ReplicaSet, Deployment )
3. "metatdata": data / information about the object ( names , lables )
       name: could be a string value ( myapp-pod)
       namespace:
       lables: (We can filter the pod )
         name:
 	     app: able to filter out the pods by giving the unique name ( myapp ) 
4. "spec": specification section , Defines whats inside the object we're creating. 
       containers: name of the pod within the container
       	 - name: nginx-container - name of the container i.e nginx container 
       	   image: nginx - docker hub image name to create 
       	   ports:
       	       - containerPort: 80
       	   env:
       	       - name: POSTGRES_USER
       	         value: "******"
       	       - name: POSTGRES_PASSWORD
       	         value: "*****"
       	  
Example 1: pod.yml
apiVersion: v1
kind: Pod
metadata:
 name: nginx
 lables: 
   app: nginx
   tier: frontend
spec:
 containers:
   - name: nginx
     image: nginx 
   - name: busybox
     image: busybox
 #kubectl create / apply -f pod.yml
 #kubectl get pods
 #kubectl describe pod nginx 
 #kubectl delete pod nginx

------------------------------------------------------------------------

------------------ Namespace ---------------
------------------ Service -----------------
------------------ Deployment --------------
 ----------------- ReplicaSet --------------
  ---C1--------C2------   ---c1-----c2------
  ---------- Pod-1-----   -----Pod2---------

-------------------: Kubernetes Controllers/ Replication Tyes :--------------------------
 Why :
     - Self Healing, if any pods goes down , bringing back is called self healing.
     - Roll-out & Roll-back, 1.21 --> 1.22 & 1.22 --> 1.21
     - High Availability
Types:
1. ReplicaSet --> desired count/replicas & actual count matches, any mismatch going to do self healing.
2. Deployment --> Gives us rollout & rollback
3. DeamonSet --> One pod per node.
4. statefulSet --> Maintain a sticky identity for each of their pods.
-------------------------------
   1. ReplicaSet (new, recommended way) , requires a "selector" definition helps to identify what pod falls under it . 
   2. Deployment 
Differences:
     Major difference between replicaset & Deployment :
       1. ReplicaSet - always makes sure desired number of replica's are always available .
       2. Deployment- except kind which is deployment . rollout & rolling updates , Whenever we create the deployment , deployment recreate the replicaset & replicaset create the pods .
   3. DeamonSet --> One pod per node.
   4. statefulSet --> Maintain a sticky identity for each of their pods.
        - Is we have stateful application like data bases, mysql,mongodb etc ... 
        - Maintain a sticky identity for each of their pods
        - with same identity. 
       when:
        - stable, unique n/w identifier. 
        - stable, persistent storage.
        - ordered, automated rolling updates
        - ordered, graceful deployment and scaling.

-----------------------: Kubernetes Services: -----------------------------------------------
# kubectl get svc --> "kubernetes" is the default service.
Why:
    - Enables the communication between various components within & outside of the application . 
Advantages:
    - Load Balancing 
    - Zero downtime 
    - service discovery
Types:
     1. Node port ( 30,000 - 32,767)
     2. ClusterIP (Port - 443 / TCP) Default one , Expose's the pod Internal to the cluster , when we don't want to expose the application to the outside world i.e database's 
     3. Load balancer
     4. ExternalName
     - map a Service to a DNS name
Multi-Port Service:
  - For some Services, you need to expose more than one port. Kubernetes lets you configure multiple port definitions on a Service object.
 
-----------------------: Kubernetes Replication Tyes : -----------------------

1. Replication Controller : One fails we still have another one , providing high availability , can help automatically bringing up the new pods even its just one or hundrade  .

Example: rc-definition.yml 
 1. apiVersion: v1
 2. kind: ReplicationController  
 3. metadata: --> for replicationcontroller
    name: myapp-rc
    lables:
  	  app: myapp
  	  type: front-end
 4. spec: --> for replicationcontroller
 	1. template: to provide the pod template. --> For Pod
 		  metadata: 
 			  name: myapp-pod
 			  lables:
 			   app: myapp
 			   type: front-end
 		  spec: --> For Pod (image,pod name, port)
 		    containers:
 		 	   - name: nginx-container
 		 	     image: nginx
           ports:
            - containerPort: 8080
 	2. replicas: 3

  # kubectl create -f rc-definition.yml
  # kubectl get replicationcontroller - to view the list of created replicationcontroller.
  # kubectl get pods
  # kubectl get all
 Note: All are starting with replicationcontroller name i.e., "myapp-rc"
 
2. ReplicaSet: replicaset-definition.yml
  - "Labels & Selectors:" Monitors the exisiting pods as its already created , if anyone fails then its deploy a new one .
  - "label" uses as filter section.
  - "selector" to connect to pod through labels.

 Example:
 1. apiVersion: apps/v1
 2. kind: ReplicaSet , "Note: R & S should be in capital"
 3. metadata:
 	  name: myapp-replicaset
 	  lables:
 		 - name: myapp
 		   type: front-end
 4. spec:
  	 	1. template:
 		    1. metadata: 
 			    name: myapp-pod
 			    lables:
 				 app: myapp
 				 type: front-end
 		   2. spec: 
 		        containers:
 		 	     - name: nginx-container
 		 	       image: nginx
      2. replicas: 3
 	    3. selector: ----> ( copy the lables from the pod definition file and paste it under the selector section )
 	  	     matchLables: 
 	  		   type: front-end

 Two ways to scale the replicaset :
 	1st: edit to 6 and use cmd #kubectl apply -f replicase-definition.yml --> Recommended Way.
 	2nd: # kubectl scale --replicas=6 -f replicas-definition.yml 
 	
3. Deployments: 
 - Ref: https://www.youtube.com/watch?v=efiMiaFjtn8
 - Rollbacks, Rollout & Versioning in a Deployments:
    - when we 1st create a deployment , it triggers a rollout , a new rollout creates a new deployment revision-1, 
    - In further when application upgraded / updated a new revision-2 created.
    - This helps us keep track changes and enables us to rollback in future if necessary .
    - Whenever we create the deployment , deployment recreate the replicaset & replicaset create the pods .

Note: When deployment gets triggered ?
 - When we're changing under "template" & "spec" section like labels / container image name. 
 - After that assigned with a new revision number.
 - If you don't define strategy , by default it's "RollingUpdate".

Status of rollout:
 # kubectl rollout status deployment/<deployment_name>  - 
 # kubectl rollout history deployment/<deployment_name>  - to see the revision & history of deployments
 # kubectl rollout undo deployment/<deployment_name>  - notice somthing change / not working after that we can use.
 - this command i.e rollback to the previous version of deployment

Deployment strategies:
 Two types
 1. Recreate, 1st Terminating previous version then recreate / deploy new application versions - Downtime
 2. RollingUpdate , default strategy, first creating the pods then terminating. 
Ex:
 strategy:
   type: RollingUpdate / Recreate
   rollingUpdate:
     maxSurge: 1 , maximum number of Pods that can be created over the desired number of Pods.
     maxunavilable: 0 ,the maximum number of Pods that can be unavailable during the update process.
----------------------------------------------------------------------
Networking in K8S:
 cisco , cilium, flannel , vmware NSX, calico (project calico)

--------------------------: Kubernetes Services: -------------------------------------------
  Enables the communication between various components within & outside of the application . 
 1. Node port ( 30,000 - 32,767) / TCP --> Stable IP Address. 
 2. Cluster IP , default one , Port - 443 / TCP
 3. Load balancer
 
 1. Node Port:
    - Exposes the Service on each Node's IP at a static port (the NodePort). 
    - A ClusterIP Service, to which the NodePort Service routes, is automatically created. 
    - You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>
  Example :
      apiVersion: v1 
      Kind: Service
      Metadata:
        name: myapp-service
      spec: 
        type: NodePort
        ports:
         - targetPort: 80 ---1 (application running - withinpod)
            port: 80 ---2 (service port)
            nodeport: 30008 ---3 (node)
       selector:  ----> "Could be hundrade of pods , so we will use lables & selector to link together " pull the lables from the pod and place under selector section . 
         app: myapp
         type: front-end
# kubectl create -f service-definition.yml
# kubectl get sv - list the service , where we can able to see cluster IP & NodePort
# curl https://192.186.1.2:30008 

2. ClusterIP: 
  - Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.
  - As Pod ip can not be static as it may goes down and again recreate with new one so, we can't relie on this ip for internal communication between the applications .
  # kubectl port-forward service/nginx-service 8083:8082

3. LoadBalancer:
 - Exposes the Service externally using a cloud provider's load balancer. NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created.
 - New load balancer created and we need to pay , When we have multiple services it will increases the cost and difficult to manage. 
 - If we're using bare metal then we've to set entrypoint like proxy-server. 
 
 Microservices Application on K8S:
  1. First we need to create the container by using the images "docker run cmd" to start an instance / container . (all images of the applications are already build and avaialble in dockerhub repo)
  2. deploy pods
  3. enable connectivity between the services . what application required .
  4. Create the services ( ClusterIP & NodePort )

----------------------------: Kubernetes Ingress: ---------------------------------------------
What is Ingress?
  - Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.
  - We can access the application from outside of the cluster. 
  - We declare which request should go to which service.So we should write the ingress rule for this. 
  - For ingress rule we should have deploy "Ingress Controller"/load balaner, pod into our cluster to process the ingress rule.
  - Ingress Controller could be an entrypoint to the cluster.

Benefits of Ingress over services:
  - Enterprise & TLS missing (Secure load balancing). 
  - If we choose service type as "LoadBalancer" cloud provider charging for each & every static IP address. 

How it Works ? 
http-->ingress managed load balancer-->ingress controller-->reads ingress rules-->forwards to service-->pod
 1. User hit the http request i.e http:/www.xyz.com
 2. Forwards the service to load balancer.
 3. Load balancer forward the service to ingress controller pod and reads the ingress rules and decide which service to call.
 4. Once decides which service to call it forwards the request to service. 
 5. And reaches to desired pod.

Imp Note:
- Now we don't need any Nodeport or load balancer services as ingress controller is a pod and can access ClusterIP services. 

Benefits over NodePort / LoadBalancer:
  - We can include our own authentication .
  - Path Based routing.
  - Host Based routing.

Ingress Controller: Mostly third parites.
Types:
  - "Nginx Ingress Controller" only maintained by K8s itself.
  - HA Proxy
  - traefik
  - F5
  - Istio etc
# kubectl get po -n ingress-nginx - to verify nginx ingress controller and get expose through service.
# kubectl get svc -n ingress-nginx - Nodeport service is created for nginx ingress controller., if cloud it should create Load balancer service. 
# kubectl describe ing nginx-ingress
kubectl get pods -A | grep nginx
kubectl logs <nginx-ingress-controller-pod.name> -n <namespace.name>
 - shows sync

Note:
 - In prod/pre-prod/dev : we use our own domain name (needs to provide under host)
Ex:
 apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wildcard-host
spec:
  rules:
  - host: "foo.bar.com"
    http:
      paths:
      - pathType: Prefix
        path: "/bar"
        backend:
          service:
            name: service1
            port:
              number: 80
TLS:
  - SSL passthrough.(not-recommended)
  - SSL Bridging
 
------------------------: Kubernetes Namespaces: ------------------------------
 1. What is Namespace?
  - Way to organise the cluster into the virtual sub-clusters, so we are creating the resources (pod,svs,rs,deployment,secrets,ingress,configs) in these namespaces instead of creating all in one namespaces. 
  - Each namespace logically separated with each other but can communicate with each other. 
  - When we create k8s cluster 4 default namespaces created ( Default, kube-node-lease, kube-public, kube-system )
       1. Default - By default resource created  within default namespace , Resource when we don't specify namespace explicitly
       2. Kube-node-lease - Contains lease resources to send the heart_beats of node , if nodes goes down , so Pods within that node will be created with healthy node . Lease will take action . 
       3. kube-public - Used for public resource . Open to all users with read only access. 
       4. kube-system - For Objects created by k8s (etcd,apiserver,kubeproxy,scheduler,control manager)
  
 2. Need of Namespace?
   - Avoid conflicts like same name between teams.
   - Restricting Access like Dev & Prod .
   - Resource limits by using resource quota , i.e allocated to the individual application according to their requirement .(CPU,RAM,Storage per NS etc)
  
 3. Namespaces in action?
   Can be created in two way
     #kubectl create namespace <nginx> , here nginx is the name of the namespace -> (1st one) 
     #kubectl get namespaces ->  2nd one - Byusing ConfigMap file ( Better recommendation )
  
  Ex: By using Configuration file.
   apiVersion: v1
   Kind: ConfigMap
   metatdata:
    name: mysql-configmap
    namespace: my-namespace
   spec:
    db_url: <>
   
   spec - is optional in namespace 
   #kubectl get all --name-namspaces or kubectl get all -A
  
==============================: Helm-3 :==========================================================
Version: 2 vs 3
 - Version2 comes in two parts (client,server model)
  - helm client 
  - server (tiller)
  - Don't have role based control access. 

 - Version3 (Client)
  - Single client architecture.
  - Releases in Nov2019.
 When we execute the cmd helm install <chartname> it ->send the request to tiller
  - Helm client & libraries written in Go language. 
---------------------------------
Like yum & apt -->package manager (automated installation, versioning, dependecies mgmt, remove etc)

What is Helm?
 - Introduce in 2015
 - Package manager for k8s.
 - To package YAML files and distributes in public & private repo.(helmhub)
 - Helm automatically maintains a database of all versions of our releases. 

What are Helm charts?
 - Bundle of YAML files.
 - We can create our own helm charts with helm
 - Download and can use.
 - With the help of helm charts we can define, install, upgrade the application. 
 Ex:
 Database apps, Mongo DB, Elasticsearch, mySQL, Prometheus etc..

Why we need helm ? / Advantages. 
 - Writing & maintaing k8s manifest file can be time consuming & tidious work. 
 - Makes application deployment easy.
 - Improves developer productivity.
 - Reduces deployment complexcity. 
 - Min three yaml files need. 
 Ex: deployment, secretes, configmaps, services etc
-------------------------
Helm chart Structure:
- Directory structure: ls
<Chart.yaml charts/ templates/ values.yaml>

  #tree mycharts
  mycharts/ -->folder
  - Charts.yaml --> meta info about chart, name, version, dependecies etc.
  - values.yaml --> Imp , place where all the values can be configure/ default value can be configured .
  - charts/ --> folder contain chart dependecies. 
  - templates/ -->folder contains actual templates files stored.
    - .txt
    - _helpers.tpl
    - deployment.yaml
    - ingress.yaml
    - service.yaml
    - serviceaccount.yaml
----------------------
Note:
- 1st values. yaml 
- values.yaml validating json schema. 
Validating charts value in JSON schema. 
------------------------
 #helm install <chartname>
  NAME:
  LAST DEPLOYED:
  NAMESPACE:
  STATUS:
  REVISION:
 #kubectl get all
-----------------------------
Heml Feature:
1. Templating Engine:
  - define a common blueprint
  - Dynamic values replaced by placeholders.
2. Same application across different clusters:
  - Ex: dev, QA, staging, prod etc.
3. Release Management: Versioning 
  - Helm automatically maintains a database of all versions of our releases. 
-------------------------------
:Examples:
---> Normal yaml file <---
apiVersion: apps/v1
kind: Deployment
metadata:
 name: Release-springboot
spec:
 replicas: 2
 selector:
  matchlabels: 

---> Helm file <---
- we need to defines values.yaml file there we need to go replicaCount, and place it here. 
apiVersion: apps/v1
kind: Deployment
metadata:
 name: Release-springboot
spec:
 replicas: {{.values.replicaCount}}
 selector:
  matchlabels: 

========================= "Livenessprob / Healthcheck" & "Readnessprob" ====================
- Application Healthcheck.
- Livenessprobes determine whether or not an application running in a container is in a healthy state or not.
- In Order to verify whether application is running or not within the container we can use livenessprob. 
- If the cmds succeds, it returns 0 and the kubelet consider the container is alive & healthy. 
- if it's get nonzero then kills the pod/container & recreate it. 
Ex:
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: mylivenessprobe
spec:
  containers:
  - name: liveness
    image: ubuntu
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 1000
    livenessProbe:                                          
      exec:
        command:                                         
        - cat                
        - /tmp/healthy
      initialDelaySeconds: 5          
      periodSeconds: 5                                 
      timeoutSeconds: 30 

  echo $? --> show health condition either 0 or 1

ReadnessProb:
- readiness probes to know when a container is ready to start accepting traffic.

========================= "K8S VOLUME" ======================================================================
Link: 
- https://www.youtube.com/watch?v=9zjGOCb-6As&list=PLBGx66SQNZ8aPsFDwb79JrS2KQBTIZo10&index=53
- https://medium.com/@saurabhdahibhate50/become-expert-of-secrets-and-configmaps-in-kubernetes-5eb0103d687e

Types:
 - cephs
 - configMap
 - secret
 - emptydir
 - PersistantVolume 
 - persistentVolumeClaim
 - local
 - nfs
 - hostpath

Node local memory (emptydir & hostPath) -->
Cloud volumes
File sharing volumes (nfs, network file system)
Distributed file systems (cephFS & glusterFS)
Special volume types (persistentVolumeClaim, secret, configMap)

How to persists data in k8s using volumes?
1. Persistent Volume (Always Available)
2. Persistent Volume Claim
3. Storage Class

- When Pod gets restarted all the data gets deleted. so need to configure database each app explicitly. 
- Storage must be available in all nodes. 
- Storage needs to survive even if cluster crashes. 

1. Persistent Volume
  - Are not namespaces. 
  - Cluster resource just like RAM CPU Use to store data
  - Created by YAML file
  - kind: PersistentVolume
  - spec: how much storage?
  - needs actual physical storage like local disk, cloud storage, nfs etc

2. Persistent Volume Claim
Pod request the volume through the PV claim, claim tries to find a volume in cluster, Volume has the actual storage backend. 
  - Claims must be in the same namespace.
  - Created by YAML file
  - kind: PersistentVolumeClaim
  - spec: 

3. Storage Class

============================: ConfigMaps & Secrets :=============================================================
4. ConfigMaps:
Why we need?
 - Application should not be hard coded, suppose user name changed, password changes 
 - so general practice is we need to save it under configMaps.
 - db port, connection type within the configMaps., as a user can't go to the container ,it's not a good practice. 
ConfigMap:
 - store the data in key-value pairs in "plain text".
 - storing the application configuration files. 
 - allows to decouple environment-specific configuration from the container image. 
Note:
 - There are four different ways that you can use a ConfigMap to configure a container inside a Pod:
 1. Inside a container command and args
 2. Environment variables for a container
 3. Add a file in read-only volume, for the application to read
 4. Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap
cmds:
 kubectl create configmap -h
 kubectl create configmap <configmap_name> --from-literal=key1=value1
  key1= username
  value1= taiz
 kubectl get cm -o wide
 kubectl describe <configmap_name> 

Interview qus: How will you use configMap inside your pods?
 - We can add VolumeMounts & Volumes, under volumes we pass the configMap name. 

5. Secrets:
 - store the data in "encrypted format", sensitive data. 
 - db username, db password etc
 - Also we can use K8s RBAC.
why not saved in configMap?
 - info saved in etcd, so data saved as object, any hacker can retrive the data.
 - to solve this problem it's store in secrets. 
Note:
 - There are three main ways for a Pod to use a Secret:
 1. As files in a volume mounted on one or more of its containers.
 2. As container environment variable.
 3. By the kubelet when pulling images for the Pod.
cmds:
 kubectl create secret -h
  - docker-registry
  - generic
  - tls
 kubectl create secret generic <secret_name> --from-literal=key1=value1
  key1= username
  value1= taiz
 kubectl get secret -o wide
 kubectl describe <secret_name> 

================: K8s RBAC :=====================================================================================
1. What is RBAC ? "Role Based Access Control"
2. How to create Users and how to do user management in Kubernetes ?
3. Kubernetes Service Accounts ?
4. Roles 
5. Role Bindings
6. Identity Providers
Note:
 - in OCP it's found under user managment. 
 - ref: openshift sandbox.
---
RBAC can be devided into two part
1. User's
  - Define Access

2. Service accounts
  - managing service's
---
Role:
 - It's a yaml file , can have the access to configmaps, secretes etc
 - Permission. 
Role Binding:
 - Once we attach to the user , its call role binding. 
Ex:
 - we need to create a role, service account and using the role binding we can attach both of them. 

=================: Custom Resource :======================================
Ref: https://www.youtube.com/watch?v=alGEPSQxbLg&list=PLdpzxOOAlwvIKMhk8WhzN1pYoJ1YU8Csa&index=51

- Whenever new resource introduce to k8s , custom resource comes into the piture.
- to extend the K8s API.
Ex: Key-clock, Argo CD etc

Three things:
1. Custom resource defination- CRD
2. Custom resources
3. Custom controllers.

======================

 
 	
 
 



