-------: GIT :----------------

---------------------------: JENKINS-2.332.3 :-----------------------------------
1. Jenkins Trigger:
 1. Build Periodically : cron job / 5 astrics, Time Interval Build 
 2. Poll scm : * * * * * , Whenever developer does the code commits automatically Builds the code
 3. Build after other projects are Build
 4. Trigger build remotely
 5. Manual 
 6. github hook trigger for git scm polling.

2. Jenkins Upstream & downstream jobs? Build - Pipeline
 - We have builds like Compile, Package & Deploy and want to run in sequence order that is executing one by one build in a sequential order).
 - Compile is called a upstream 
 - Deploy is called as downstream.

3. Multibranch Pipeline with a Webhook on Jenkins.
 - Sometimes it may be necessary to create a pipeline on Jenkins for each Git branch. In this case it could be difficult to create independent pipeline for each branch. 
 - Besides that what if we create or delete a branch in future? So someone has to take care of the pipelines on Jenkins whenever there is change in branches. 
 - That’s where Multibranch pipeline comes into picture.
 - Through Multibranch pipeline we can create a pipeline for each branch in a repository and it also create or remove when there is a change in the branches.
Create a Multibranch Pipeline
 - Login to Jenkins GUI
 - Click on “New Item” → Specify a job name → Select “Multibranch Pipeline option”
 - Give Display Name and Description
 - Under Branch Sources → Add source → Chose Git → and provide GitHub URL and Credentials (Credentials are optional if it is public repo)
 - Under Build Configuration → chose Jenkinsfile path. Most of the case it will be under Repo root directory.
 - Under "Behaviour" select branch name or we can add in Jenkinsfile with when expression

Apply and Save the job.
Now Jenkins automatically scans the repository and create a job for each branch wherever it finds a Jenkinsfile and initiate first build.

Using Webhook:
 - If you wish to automate the build process in the multibranch pipeline we can use Webhook. This feature is not enabled until we install “Multibranch Scan Webhook Trigger”.
 - This enables an option “scan by webhook” under “Scan Multibranch Pipeline Triggers”. Here we should give a token.
 - Now to enable auto build process we should provide Jenkins URL with token in the GitHub. 
 - For this log into GitHub → settings → Webhooks → Add webhook.
 - Provide Payload URL and Content type as “application/json” and click on Add webhook

4. Jenkins Pipeline: 
 - In Jenkins pipeline is a collection of events / Jobs which are interlinked with one another in a sequence.
 - Jenkins pipeline is a “suite of plugins” which supports implementing & integrating “continuous delivery pipelines” into Jenkins.
 - We integrate pipelines into Jenkins either “Plugins” or “scripts”.
 - It starts from version control to the end users and customers.
 - Jenkins pipelines written in file called “Jenkinsfile”.
 - Pipeline defines the entire build process which included building the application, testing & delivering it.
Two ways to create Jenkins file:
 1. Pipeline script.
 2. pipeline script from SCM.
Pipeline Syntax: Two types:
 - Scripted (1st syntax , Groovy engine , advance scripting capabilities , high flexibility )
 - Declarative (recent addition , easier to get started , pre-defined structure )
Declarative Pipeline:
 - “Pipeline”  must be top level
 - “Agent”
 - “Stages” : where the whole work happens.
 - Under stages we have “stage” i.e build test package deploy(.jenkins/workspace) etc
 - Under stage we have “steps” (script ), sh rm –rf , sh git clone , sh mvn clean etc

5. CONTINOUS INTEGRATION: build stage to Testing Stage. 
 - Continuous integration (CI) is an automated process in DevOps which generate software and its features quickly and efficiently.
 - Developers write the code and stores the code in the centralized repository called version control system i.e in GitHub, Code commit happens continuously. 
 - Code will move to build server where code build, tested and evaluated which generates the artifact like war/jar and stores in artifact repo like Nexus/Jfrog . 
 - A successful CI build may lead to further stages of continuous delivery.
 - If a build fails, the CI system blocks it from progressing to further stages. The team receives a report and repairs the build quickly, typically within minutes.
Benefits of CI:
 - Improved developer productivity
 - Find bugs earlier, fix them faster

Continuous Delivery & Continuous Deployment:
 - Continuous delivery is an extension of continuous integration since it automatically deploys all code changes to a SIT,Dev,LUAT  environment after the build stage.
 - In delivery, there is a final manual approval required before pre-prod & production release. 
 - In the deployment phase, the package is opened and reviewed with a system of automated checks. If the checks fail the package is rejected. 
 - When the checks pass the package is automatically deployed to production. Continuous deployment is the full end to end, automated software deployment pipeline.

6. PROJECT-1  JENKINS-ANSIBLE-NODE / TOMCAT SERVER:
PREREQUISITES: Jenkins server, Ansible server, Tomcat/ansible node
 - Connect tomcat server with Ansible mster then Ansible mstr with Jenkins server.
 - In Jenkins Manage Plugins install “Publish Over SSH”. Install without restart.
 - Publish Over SSH: path to key: /root/.ssh/ id_rsa (private key)
 - ADD server- Server name: Ansible mstr hostname, 
 - Hostname: Public IP, 
 - Usrname: root, 
 - Remote directory: /opt   “Test Configuration – Success or Not” 
Choose Freestyle Project
 - SCM: Github URL
 - Build: Invoke Top Level Maven Target
 - Maven-goals (clean compile test package install)
Write the playbook: In Ansible Mstr
Host: all
Gather_facts: yes
Become: true install jdk & tomcat8, through copy module src file and destination path (var/lib/tomcat/webapps)

 - After compile “Post Build Action”-send build artifact over SSH-
 - Transfer: source file, .war file then remote directory path->
 - Execute command: ansible-playbook playbooks/pb1.yml
 - Ansible Mstr to Tomcat server/Nodes:

7. JENKINS & SLAVE CONFIGURATION:
 Jenkins Server & Jenkins salve Node Communicate with SSH config.
 - In slave have to create workspace /opt/.jenkins
 - Slave node install java 
 - Jenkins server – Manage Nodes & Clouds , New Node , remote root directory - /opt/.jenkins
 - “Restrict where this project can be run”

8. JENKINS ROLES BASED AUTHORISE STRATEGY?
 - Manage User: Create the user
 - Manage Jenkins: manage plugins: “role based auth strategy “
 - Configure global security: Authorization: “Role based strategy “: SAVE
 - Manage and assign roles: Manage roles: roles to add

9. JENKINS BUILD FAILURES:
 - Disk full
 - Mvn goals – clean compile install: spelling error 
 - Compile Errors- Developers 
 - pom.xml - distribution management , pulls any dependecies etc
 - 

PRE-BUILD CHECKS:
 - Mock builds
 - Proper software version checks
 - Disk sizes
 - Check whether machine is reachable or not.
 - Network Intimate Issue

10. JEKINS BACKUP:
 - Manage Jenkins- Plugins- “thinbackup”, enter
 - Setting- Configure,
backup directory path: /tmp/backup1
 - backup schedule – Using Cron tab
 - Max Number of backups – 2/3
Click On Backup Now & Restore Now.

11. Jenkins Env Variables.
 - BRANCH_NAME
 - CHANGE_ID
 - CHANGE_URL
 - BUILD_ID
 - JOB_NAME
 - BUILD_TAG

-------: MAVEN :--------------
 Refere to Maven File.

----------------------: ANSIBLE :---------------------------------------
1. Ansible Roles: 
 - If we add more and more functionalities to a single playbook it will make it difficult to maintain in a single file.
 - To resolve this issue, the playbooks are arranged in a directory structure called roles. 
 - Under roles directory we created sub directory such as files, handlers, meta, task, vars) and inside sub directory We create main.yml file.
 - When we call main playbook, main playbook will call all sections files respectively in the order whatever  we mention in playbook.
 - So, by using this Role, we can maintain small playbook without any complexity.
 - Roles 
    - FILES (index.html / sample.html / war )
    - HANDLERS (main.yml) apche2 ,tomcat 8 , mysql_server start
    - META
    - TASKS ( deploy_app.yml , main .yml , pack_install.yml )
    - VARS (main.yml ) , pack1/pack2 /pack3     
 Hosts: all
 Become : true
 Gather_facts: true
 Roles:
  -myrole    

2. Ansible Tower
 - Formerly known as AWX, Now called as Red hat Ansible , web based GUI version. 

3.Ansible Galaxy
 - Galaxy is a repository of Ansible roles that can be shared among users and can be directly dropped into playbooks for execution. 
 - It is also used for the distribution of packages containing roles, plugins, and modules also known as collection.
 - The ansible-galaxy-collection command implements similar to init, build, install, etc like an ansible-galaxy command.
 - Repository for “Ansible Roles”
 - Docker: Geerlingguy
 - #ansible-galaxy search , #ansible-galaxy install 
 Example: 

4.  Ad-Hoc commands:
 - Ad Hoc commands are ready to use commands using to perform quick task, without using playbooks we can use these Ad-Hoc commands for temporary purpose.
 - These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. 
 - Here we don’t use Ansible modules. So Idem-potency will not work with Ad-Hoc commands. 
 - If at all we don’t get required YAML module to write to create infrastructure, then we go for it. 
Ex:
 - ansible-all -i hosts -l ubnt -m apt -a "name=apache2 state=present"
 - ansible-all -i hosts -l <ip> -m setup -a "filter=ansible_distribution"
 - ansible-all -m package -a "name=net-tools state=latest"

5. When we run playbook what are the things happening?
 - Gathering Facts
 - If update repo then showing update repo
 - To install pack
 - To deploy file
 - To start services 

6. Suppose run the playbook and error comes how to omit?
 - Ignore_error = yes , If wants to install many packs or again verify the playbook .

7. Why only Ansible ?
 - Agent less only single master
 - Using Push Model
 - Using YAML/python based language which is human readable 
 - highly flexible and Ready to use modules
 - free and open source

8. Structure of Ansible
 - Modules: Pre defined
 - Module utilities
 - Plugins: email,logging, others
 - Inventory: Host inventory i.e node details
 - Playbooks: scripts
 - Connection plugins to Nodes via SSH

9.  Ansible How to connect Nodes with Master.
 NODE:  Through SSH (Secure shell configuration)
 - To change the root password – passwd root
 - To configure SSH (vi /etc/ssh/sshd_config -> root permit login & password authentication => YES)
 - Restart the SSH (service SSH restart, systemctl restart sshd)
 - Connect with master through private IP .
MASTER:
 - to generate key pair: SSH-KEYGEN
 - to send public key to nodes: SSH-COPY-ID <NODE IP>
 - able to connect through SSH 
Ansible-Installation :
     apt-get update -y
     apt-get install software-properties-common -y
     apt-add-repository ppa:ansible/ansible
     apt-get update
     apt-get install ansible -y
     ansible version: 2.9.22 , python: 2.7

10. Ansible Modules?
 - Yum, Apt, Get_url,  File, Ping , Package, Raw ,Group, User, Git, 
 - Unarchive, Copy, Template, Shell, Setup, Debug, cron.
 - shell module:

11. Ping Module?
 - Check whether Master is reachable to respective nodes or not and response getting as “Pong”.
 - ansible all -m ping

12. Plugins using-
 - publish over SSH : Ansible 
 - Maven integration: Maven
 - Build Pipeline 
 - Role based Authorized strategy

13. Ansible playbook structure?
 - HOST : ubnt/centos
 - USER : remote_user: root
 - VARIABLE: section, var1/var2 (pkg )
 - TASKS: each play contains task and these task from top to bottom,under task we use modules. 
 - HANDLERS: to start the services

14. Ansible Dryrun? -Check --diff
 - Dry run called “Check mode” to check the playbook before run. 
 - Ansible Dry Run feature we can execute the playbook without having to actually make changes on the server.
 - #ansible-playbook pb1.yml –check –diff (what needs to be change)
 - #ansible-playbook pb1.yaml --syntax-check

15. Ansible Idempotency:
 - For Ansible it means After first run of a playbook to set things to a desired state. 
 - Further runs of the same playbook should result in 0 changes.
 - In simplest terms, idempotency means you can be sure of a consistent state in your environment.

16. Ansible Config file?
 

-----------------------------------: DOCKER :-----------------------------------
1.  VM's                      Container
 - More time to create      - Less time to create
 - Dedicated OS             - Common OS
 - Specific bin/lib         - Common bin/lib
 - Memory wastage           - No memory wastage
 - Low Performance          - High performance
 - Height weight m/c        - Light weight m/c
 - Complex configuration    - Less configuration
 - Memory can't share       - memory can share

2. How to check multiple running containers CPU Usages:
 - # docker stats  --all
 - # docker stats <containerID>

3. Docker -itd vs -it?
 -itd: interactive terminal in detached mode, running in backend with /bin/bash. 
 -it: interactive terminal, container will create and we are entered into that container.

4. Docker Kill vs Stop?
 - STOP: Stop the running container and send the signal SIGTERM.	
 - Kill:  Kill one or more containers and send the signal SIGKILL.

5. Docker Daemon?
 - A docker Deamon (Dokered ) listens for Docker API requests and manages Docker objects such as images, containers, volumes etc. 
 - Also it can communicate with other Docker Deamon to manage services.

6. Docker exec ?
 - Docker run using to to lunch the new container 
 - Docker exec runs a new command line in the running container.

7. How to get an Docker Image?
 - Docker pull <imagename>: pull an image from docker hub
 - Docker commit:  convert a container into an image, docker commit  <container id> myimage
 - Docker build: build a script from docker file.

8. Docker Port Mapping: 
  - docker pull nginx
  - docker run -d --name <name> -p 8080:80 nginx:latest
 Docker Volume: Copy the data from local to nginx container: 
  - docker run --name website -v <desktop_path>:</usr/share/nginx/html> -d -p 8080:80 nginx:latest
  - docker exec -it <containername/containerID> bash

9. Docker File??
 - Create an image automatically using a build script.
Ex:
 - Vi Dockerfile
 - FROM ubuntu
 - MAINTAINER xyz
 - RUN apt-get update
 - RUN echo “HI”
 - CMD [ “echo” “HELLO WORLD”]
 - #docker build –t myimage:1.0 .
 - #docker run -d --name <name> myimage:1.0

10. DOCKER ContainerIP?
 - 172.17.0.1: host
 - docker ispect <containerID>
 
11. DOCKER FILE: Single vs Multiple "RUN" commands ?
 - After building the image all layers stored under "Docker Cache", Suppose we later modify apt-get install by adding extra pkg
 - Docker sees the initial & modified instructions as identical & reuses the cache from previous steps.
 - As a result the apt-get is not executed bcz the build uses the cached version. 
 - So build can potentially get an outdated version of the curl / nginx pkg. 
Using RUN apt-get update && apt-get install -y nginx , ensures the Dockerfile installs latest pks. 
 - This technique is known as "cache busting"

--------: K8S :---------------

--------: LINUX :-------------
1. Application port is running or not ?
 - command using 'netstat -lntp'
 - ex- tomcat 8080,nginx 80

2. Memory Check                                                       
 - "TOP" Able to see the CPU Usages 
 - "free -h" primary memory RAM Usages
 - "df -h" secondary memory HDD Usages
 - "du" directory memory check. 
 - uptime: Load Average

3. SOFT LINK VS HARD LINK & INODE IN LINUX:
 - An inode is a data structure that stores various information about a file in Linux,
   Containing information (the metadata) about a regular file and directory.
   Inode: Uniqe number for every file in Linux
 - Soft lInk: ln -s demo.txt Slink , only store the path, no data can store here , different inode value
 - Hard Link: ln demo.txt Hlink , contains data , replica of actual file, similar Inode value, if file is deleted can access the data from Hlink.

4. LINUX BOOTSTRAP PROCESS:
  BIOS: Basic Ip/op system , executes MBR
  MBR: Master boot record , executes GRUB
  GRUB: Grand Unified boot loader , executes kernel
  KERNEL: executes /sbin/init
  INIT: executes run level program
  RUNLEVEL:

5. Linux Run-lables.
 0 shutdown
 1 single user
 2 multiuser
 3 multiuser with n/wing
 4 notused
 5 GUI
 6 Reboot