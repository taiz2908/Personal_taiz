1. How to Pass DevOps Interview | DevOps Interview Tips | How to Ace DevOps Interview | SRE Interview:
https://www.youtube.com/watch?v=JLU_CHe3azY

2. 
----------: Ports & Versions: ----------------------
Tomcat:
Nginx:
Git: v2.38.1
Jenkins: v2.332.3
Sonarqube:
K8s:
Docker:
Helm: v3
Maven:
Ansible:
Terraform: v1.3.4

------------------------ DOCKERFILE'S EXAMPLE ----------------------------------------
Docker file's:
1. Building docker file & create a nginx image. 
 mkdir folder_name
 docker build --help
 vi Dockerfile
 FROM nginx:latest
 ADD . /usr/share/nginx/html

 # docker build -t api-website:latest .
 # docker tag api-website:latest api-website:1 , changing a latest tag to 1 / renaming a tag
 # docker image ls 
 # docker run --name website -p 8080:80 -d website:latest
 # docker ps 
 # docker exec -it <containerID_or_conatinerNAME> /bin/sh , enter inside the container. 

2. Springboot application + Tomcat Image:
  FROM tomcat:lates8.5-jdk11-openjdk-slim
  RUN rm -rf /usr/local/tomcat/webapps/*
  ADD target/sample.war /usr/local/tomcat/webapps
  EXPOSE 8080
  CMD ["catalina.sh", "run"]

3. Building NodeJs & ExpressJs
 FROM node:alpine
 WORKDIR /app
 ADD package*.json ./
 RUN npm install
 ADD . .
 CMD node index.js

4. Building tomcat image and using copy,add,expose,cmd instruction. 
   FROM tomcat:8.0
   MAINTAINER satish
   RUN apt-get update && apt-get install -y curl
   RUN apt-get install -y git-core
   COPY index.html /usr/local/tomcat/webapps/ROOT/index.html
   ADD https://tomcat.apache.org/tomcat-7.0-
doc/appdev/sample/sample.war /usr/local/tomcat/webapps/sample.war
   USER root
   WORKDIR /usr/local/tomcat/webapps
   EXPOSE 8080
   CMD ["catalina.sh","run"]

5. Taking linux OS & building nginx image.
  FROM ubuntu
  MAINTAINER satish
  RUN apt-get update && apt-get install -y nginx
  COPY index.html /usr/share/nginx/html/index.html
  USER root
  WORKDIR /usr/share/nginx/html
  ENV JAVA_PATH='/usr/bin/java'
  VOLUME /myvol
  EXPOSE 80
  CMD ["service","nginx","start"]

6. Create a dockerfile for nginx and copy html file to target path and open the port.
  FROM nginx:latest
  COPY ./index-html/*  /usr/share/nginx/html/
  EXPOSE 80
  # docker build -t web:latest .
  # docker run -d --name nginxwebb -p 8089:80 -v myvol:/usr/share/nginx/html nginxweb

=============================: GIT :=============================================
1. What is Git stash?
 - We create multiple branches to work simultaneously on multiple features. But to work on multiple tasks simultaneously in one branch (i.e. on one feature), we use git stash. Stash is a temporary repository where we can store our content and bring it back whenever we want to continue with our work with that stored content.
 - It removes content inside file from working directory and puts in stashing store and gives clean working directory so that we can start new work freshly.
 - Later on you can bring back that stashed items to working directory and can resume your work on that file. Git stash applicable to modified files. Not new files. Once we finish our work, we can remove all stashed items form stash repository.

2. What is Git Reset?
 - Git Reset command is used to remove changes form staging area. This is bringing back file form staging area to work directory. We use this command before commit. Often we go with git add accidentally. In this case if we commit, that file will be committed. Once you commit, commit ID will be generated and it will be in the knowledge of everyone. So to avoid this one, we use Git reset.
 - If you add “–hard” flag to git reset command, in one go, file will be removed from staging area as well as working directory. We generally go with this one if we fell that something wrong in the file itself.

3. What is Git Revert?
 - Git Revert command is used to remove changes from all 3 stages (work directory, staging area and local repository). We use this command after commit. Sometimes, we commit accidentally and later on we realize that we shouldn’t have done that. For this we use Git revert. This operation will generate new commit ID with some meaningful message to ignore previous commit where mistake is there. But, here we can’t completely eliminate the commit where mistake is there. Because Git tracks each and every change.

4. Difference between Git pull and Git clone?
 - We use these two commands to get changes from central repository. For the first time if you want whole central repository in your local server, we use git clone. 
 - It brings entire repository to your local server. Next time onwards you might want only changes instead of whole repository. In this case, we use Git pull.
 - Git clone is to get whole copy of central repository
 - Git pull is to get only new changes from central repository (Incremental data)

5. What is the difference between Git pull and Fetch?
 - We use Git pull command to get changes from central repository. In this operation, internally two commands will get executed. One is Git fetch and another one is Git merge.
 - So Git pull is the combination of {Git pull & Git merge}.
 - Git fetch means, only bringing changes from central repo to local repo. But these changes will not be integrated to local repo which is there in your server.
 - Git merge means, merging changes to your local repository which is there in your server. Then only you can see these changes.
 
6. What is the difference between Git merge and rebase?
 - We often use these commands to merge code in multiple branches. Both are almost same but few differences. When you run Git merge, one new merge commit will be generated which is having the history of both development branches. 
 - It preserves the history of both branches. By seeing this merge commit, everyone will come to know that we merged two branches. 
 - If you do Git rebase, commits in new branch will be applied on top of base branch tip. 
 - There won’t be any merge commit here. It appears that you started working in one single branch form the beginning. 
 - This operation will not preserves the history of new branch.

7. What is Git Bisect?
 - Git Bisect we use to pick bad commit out of all good commits. 
 - Often developers do some mistakes. For them it is very difficult to pick that commit where mistake is there.
 - They go with building all commits one by one to pick bad commit. But Git bisect made their lives easy. Git bisect divides all commits equally in to two parts (bisecting equally). Now instead of building each commit, they go with building both parts. Where ever bad commit is there, that part build will be failed. We do operation many times till we get bad commit. So Git bisect allows you to find a bad commit out of good commits. You don’t have to trace down the bad commit by hand; git-bisect will do that for you.

8. What is Git squash?
 - To move multiple commits into its parent so that you end up with one commit. If you repeat this process multiple times, you can reduce “n” number of commits to a single one. Finally we will end up with only one parent commit. We use this operation just to reduce number of commits.

9. What is Git hooks?
 - We often call this as web hooks as well. By default we get some configuration files when you install git. These files we use to set some permissions and notification purpose. 
 - We have different types of hooks (pre commit hooks & post commit hooks)
 - Pre-commit hooks:- Sometimes you would want every member in your team to follow certain pattern while giving commit message. Then only it should allow them to commit.
 - These type of restrictions we call pre-commit hooks.
 - Post-commit hooks:- Sometimes, being a manager you would want an email notification regarding every commit occurs in a central repository. This kind of things we call post-commit hooks.
 - In simple terms, hooks are nothing but scripts to put some restrictions.

10. What is Git cherry-pick?
 - When you go with git merge, all commits which are there in new development branch will be merged into current branch where you are. 
 - But sometimes, requirement will be in such that you would want to get only one commit form development branch instead of merging all commits. 
 - In this case we go with git cherry-pick. Git cherry-pick will pick only one commit whatever you select and merges with commits which are there in your current branch. 
 - So picking particular commit and merging into your current branch we call git cherry-pick.

11. How to make a changes for a commit done before 30days? 
 - git clone -> create a branch -> applies our changes
 - git log --oneline
 - git log --since=03.03.2023
 - git log --before=03.03.2023

12. What is Branch protection in github ?
 - setting, click on branches, then branch protection, click on add rule. 
 - Require a pull request before merging 
 - Require status checks to pass before merging

---------------------------: JENKINS-2.332.3 :-----------------------------------
1. Jenkins Trigger:
 1. Build Periodically : cron job / 5 astrics, Time Interval Build 
 2. Poll scm : * * * * * , Whenever developer does the code commits automatically Builds the code
 3. Build after other projects are Build
 4. Trigger build remotely
 5. Manual 
 6. github hook trigger for git scm polling.

2. Jenkins Upstream & downstream jobs? Build - Pipeline
 - We have builds like Compile, Package & Deploy and want to run in sequence order that is executing one by one build in a sequential order).
 - Compile is called a upstream 
 - Deploy is called as downstream.

3. Multibranch Pipeline with a Webhook on Jenkins.
 - Sometimes it may be necessary to create a pipeline on Jenkins for each Git branch. In this case it could be difficult to create independent pipeline for each branch. 
 - Besides that what if we create or delete a branch in future? So someone has to take care of the pipelines on Jenkins whenever there is change in branches. 
 - That’s where Multibranch pipeline comes into picture.
 - Through Multibranch pipeline we can create a pipeline for each branch in a repository and it also create or remove when there is a change in the branches.

Create a Multibranch Pipeline: 
 - Login to Jenkins GUI
 - Click on “New Item” → Specify a job name → Select “Multibranch Pipeline option”
 - Give Display Name and Description
 - Under Branch Sources → Add source → Chose Git → and provide GitHub URL and Credentials (Credentials are optional if it is public repo)
 - Under Build Configuration → chose Jenkinsfile path. Most of the case it will be under Repo root directory.
 - Under "Behaviour" select branch name or we can add in Jenkinsfile with when expression

Apply and Save the job.
 - Now Jenkins automatically scans the repository and create a job for each branch wherever it finds a Jenkinsfile and initiate first build.

Using Webhook:
 - If you wish to automate the build process in the multibranch pipeline we can use Webhook. This feature is not enabled until we install “Multibranch Scan Webhook Trigger”.
 - This enables an option “scan by webhook” under “Scan Multibranch Pipeline Triggers”. Here we should give a token.
 - Now to enable auto build process we should provide Jenkins URL with token in the GitHub. 
 - For this log into GitHub → settings → Webhooks → Add webhook.
 - Provide Payload URL and Content type as “application/json” and click on Add webhook

4. Jenkins Pipeline: 
 - In Jenkins pipeline is a collection of events / Jobs which are interlinked with one another in a sequence.
 - Jenkins pipeline is a “suite of plugins” which supports implementing & integrating “continuous delivery pipelines” into Jenkins.
 - We integrate pipelines into Jenkins either “Plugins” or “scripts”.
 - It starts from version control to the end users and customers.
 - Jenkins pipelines written in file called “Jenkinsfile”.
 - Pipeline defines the entire build process which included building the application, testing & delivering it.
Two ways to create Jenkins file:
 1. Pipeline script.
 2. pipeline script from SCM.
Pipeline Syntax: Two types:
 - Scripted (1st syntax , Groovy engine , advance scripting capabilities , high flexibility )
 - Declarative (recent addition , easier to get started , pre-defined structure )
Declarative Pipeline:
 - “Pipeline”  must be top level
 - “Agent”
 - “Stages” : where the whole work happens.
 - Under stages we have “stage” i.e build test package deploy(.jenkins/workspace) etc
 - Under stage we have “steps” (script ), sh rm –rf , sh git clone , sh mvn clean etc

5. CONTINOUS INTEGRATION: build stage to Testing Stage. 
 - Continuous integration (CI) is an automated process in DevOps which generate software and its features quickly and efficiently.
 - Developers write the code and stores the code in the centralized repository called version control system i.e in GitHub, Code commit happens continuously. 
 - Code will move to build server where code build, tested and evaluated which generates the artifact like war/jar and stores in artifact repo like Nexus/Jfrog . 
 - A successful CI build may lead to further stages of continuous delivery.
 - If a build fails, the CI system blocks it from progressing to further stages. The team receives a report and repairs the build quickly, typically within minutes.
Benefits of CI:
 - Improved developer productivity
 - Find bugs earlier, fix them faster

Continuous Delivery & Continuous Deployment:
 - Continuous delivery is an extension of continuous integration since it automatically deploys all code changes to a SIT,Dev,LUAT  environment after the build stage.
 - In delivery, there is a final manual approval required before pre-prod & production release. 
 - In the deployment phase, the package is opened and reviewed with a system of automated checks. If the checks fail the package is rejected. 
 - When the checks pass the package is automatically deployed to production. Continuous deployment is the full end to end, automated software deployment pipeline.

6. PROJECT-1  JENKINS-ANSIBLE-NODE / TOMCAT SERVER:
PREREQUISITES: Jenkins server, Ansible server, Tomcat/ansible node
 - Connect tomcat server with Ansible mster then Ansible mstr with Jenkins server.
 - In Jenkins Manage Plugins install “Publish Over SSH”. Install without restart.
 - Publish Over SSH: path to key: /root/.ssh/ id_rsa (private key)
 - ADD server- Server name: Ansible mstr hostname, 
 - Hostname: Public IP, 
 - Usrname: root, 
 - Remote directory: /opt   “Test Configuration – Success or Not” 
Choose Freestyle Project
 - SCM: Github URL
 - Build: Invoke Top Level Maven Target
 - Maven-goals (clean compile test package install)
Write the playbook: In Ansible Mstr
Host: all
Gather_facts: yes
Become: true install jdk & tomcat8, through copy module src file and destination path (var/lib/tomcat/webapps)

 - After compile “Post Build Action”-send build artifact over SSH-
 - Transfer: source file, .war file then remote directory path->
 - Execute command: ansible-playbook playbooks/pb1.yml
 - Ansible Mstr to Tomcat server/Nodes:

7. JENKINS & SLAVE CONFIGURATION:
 Jenkins Server & Jenkins salve Node Communicate with SSH config.
 - In slave have to create workspace /opt/.jenkins
 - Slave node install java 
 - Jenkins server – Manage Nodes & Clouds , New Node , remote root directory - /opt/.jenkins
 - “Restrict where this project can be run”

8. JENKINS ROLES BASED AUTHORISE STRATEGY?
 - Manage User: Create the user
 - Manage Jenkins: manage plugins: “role based auth strategy “
 - Configure global security: Authorization: “Role based strategy “: SAVE
 - Manage and assign roles: Manage roles: roles to add

9. JENKINS BUILD FAILURES:
 - Disk full
 - Mvn goals – clean compile install: spelling error 
 - Compile Errors- Developers 
 - pom.xml - distribution management , pulls any dependecies etc
 - 

PRE-BUILD CHECKS:
 - Mock builds
 - Proper software version checks
 - Disk sizes
 - Check whether machine is reachable or not.
 - Network Intimate Issue

10. JEKINS BACKUP:
 - Manage Jenkins- Plugins- “thinbackup”, enter
 - Setting- Configure,
backup directory path: /tmp/backup1
 - backup schedule – Using Cron tab
 - Max Number of backups – 2/3
Click On Backup Now & Restore Now.

11. Jenkins Env Variables.
 - BRANCH_NAME
 - CHANGE_ID
 - CHANGE_URL
 - BUILD_ID
 - JOB_NAME
 - BUILD_TAG

12. How to take Backup & restore jenkins configuration and data?
 - backup the jenkins plugins by copying the "plugins" directory in the jenkins home dir to the another location.
 - Backup config files "jenkins.xml" & "secrets" by copying them to another location.
 - we can copy the "job" dir "userContent" dir to another location.

13. Configure new jenkins pipeline but getting error says "no tool maven found", how to resolve this issue?
 - Jenkins unable to locate maven installation directory. 
 - need to provide correct path in jenkins.
 - we need to go to jenkins dashboard-->manage jenkins-->global tool configuration-->add maven --> provide the path.
 - MAVEN_HOME path need to provide.

14. You have a jenkins pipeline that runs unit test for your app, but the pipeline fails when the test fails, 
How would you configure the pipeline to continue running even after is a test fails?
 - We need to use the try-catch block in jenkinsfile.
 - In try block - we need to add jenkinsfile that contains a cmd to run the unit test.
 - Add catch block - will execute if a test fails.
----------------------: ANSIBLE :---------------------------------------
1. Ansible Roles: 
 - If we add more and more functionalities to a single playbook it will make it difficult to maintain in a single file.
 - To resolve this issue, the playbooks are arranged in a directory structure called roles. 
 - Under roles directory we created sub directory such as files, handlers, meta, task, vars) and inside sub directory We create main.yml file.
 - When we call main playbook, main playbook will call all sections files respectively in the order whatever  we mention in playbook.
 - So, by using this Role, we can maintain small playbook without any complexity.
 - Roles 
    - FILES (index.html / sample.html / .war )
    - HANDLERS (main.yml) apche2 ,tomcat 8 , mysql_server start
    - META
    - TASKS ( deploy_app.yml , main .yml , pack_install.yml )
    - VARS (main.yml ) , pack1/pack2 /pack3     
 Hosts: all
 Become : true
 Gather_facts: true
 Roles:
  -myrole    

2. Ansible Tower
 - Formerly known as AWX, Now called as Red hat Ansible , web based GUI version. 

3.Ansible Galaxy
 - Galaxy is a repository of Ansible roles that can be shared among users and can be directly dropped into playbooks for execution. 
 - It is also used for the distribution of packages containing roles, plugins, and modules also known as collection.
 - The ansible-galaxy-collection command implements similar to init, build, install, etc like an ansible-galaxy command.
 - Repository for “Ansible Roles”
 - ansible-galaxy search  
 - ansible-galaxy install
 - ansible-galaxy role init kubernetes 
 Example: 

4. Ad-Hoc commands:
 - Ad Hoc commands are ready to use commands using to perform quick task, without using playbooks we can use these Ad-Hoc commands for temporary purpose.
 - These are simple one liner Linux commands we use to meet temporary requirements without actually saving for later. 
 - Here we don’t use Ansible modules. So Idem-potency will not work with Ad-Hoc commands. 
 - If at all we don’t get required YAML module to write to create infrastructure, then we go for it. 
Ex:
 - ansible all -i hosts -l <ubnt> -m apt -a "name=apache2 state=present"
 - ansible all -i hosts -l <ip> -m setup -a "filter=ansible_distribution"
 - ansible all -i hosts -l <centos> -m package -a "name=net-tools state=latest"

5. When we run playbook what are the things happening?
 - Gathering Facts
 - If update repo then showing update repo
 - To install pack
 - To deploy file
 - To start services 

6. Suppose run the playbook and error comes how to omit?
 - Ignore_error = yes , If wants to install many packs or again verify the playbook .

7. Why only Ansible ?
 - Agent less only single master
 - Using Push Model
 - Using YAML/python based language which is human readable 
 - highly flexible and Ready to use modules
 - free and open source

8. Structure / Architechture of Ansible.
 - Modules: Pre defined
 - Module utilities
 - Plugins: email,logging, others
 - Inventory: Host inventory i.e node details
 - Playbooks: scripts
 - Connection plugins to Nodes via SSH

9.  Ansible How to connect Nodes with Master.
 NODE:  Through SSH (Secure shell configuration)
 - To change the root password – passwd root
 - To configure SSH (vi /etc/ssh/sshd_config -> root permit login & password authentication => YES)
 - Restart the SSH (service SSH restart, systemctl restart sshd)
 - Connect with master through private IP .
MASTER:
 - to generate key pair: SSH-KEYGEN
 - to send public key to nodes: SSH-COPY-ID <NODE IP>
 - able to connect through SSH 
Ansible-Installation :
     apt-get update -y
     apt-get install software-properties-common -y
     apt-add-repository ppa:ansible/ansible
     apt-get update
     apt-get install ansible -y
     ansible version: 2.9.22 , python: 2.7

10. Ansible Modules?
 - Yum, Apt, Get_url,  File, Ping , Package, Raw ,Group, User, Git, 
 - Unarchive, Copy, Template, Shell, Setup, Debug, cron.
 - shell module:

11. Ping Module?
 - Check whether Master is reachable to respective nodes or not and response getting as “Pong”.
 - ansible all -m ping

<!-- 12. Plugins using-
 - publish over SSH : Ansible 
 - Maven integration: Maven
 - Build Pipeline 
 - Role based Authorized strategy --> 

13. Ansible playbook structure?
 - HOST : ubnt/centos
 - USER : remote_user: root
 - VARIABLE: section, var1/var2 (pkg )
 - TASKS: each play contains task and these task from top to bottom,under task we use modules. 
 - HANDLERS: to start the services
Ex: 1,
---
- hosts: ubnt
  become: true
  gather_facts: yes
  tasks:
  - name: to update repo
    raw: apt-get -y update

  - name: to install apache
    apt: name=apache2 state=present

  - name: to deploy a file
    copy:
      src: /root/files/index.html
      dest: /var/www/html/index.html

  - name: to start service
    service: name=apache2 state=started

Ex:2, 
---
- hosts: all
  become: true
  gather_facts: yes
  vars:
  - pack1: mysql-server
  - pack2: mariadb-server
  tasks:
  - name: to update and install {{pack1}}
    package: name={{pack1}} update_cache=yes state=latest
    when: ansible_distribution=="Ubuntu"
    notify:
      - start mysql

  - name: to update and install {{pack2}}
    package: name={{pack2}} update_cache=yes state=latest
    when: ansible_distribution=="CentOS"
    notify:
      - start mariadb
  handlers:
  - name: start mysql
    service: name=mysql state=started
  - name: start mariadb
    systemd: name=mariadb state=started

14. Ansible Dryrun? -Check --diff
 - Dry run called “Check mode” to check the playbook before run. 
 - Ansible Dry Run feature we can execute the playbook without having to actually make changes on the server.
 - # ansible-playbook pb1.yml -check –-diff (what needs to be change)
 - # ansible-playbook pb1.yaml --syntax-check

15. Ansible Idempotency:
 - For Ansible it means After first run of a playbook to set things to a desired state. 
 - Further runs of the same playbook should result in 0 changes.
 - In simplest terms, idempotency means you can be sure of a consistent state in your environment.
 
16. ansible-playbook offers five tag-related command-line options:
  --tags all - run all tasks, ignore tags (default behavior)
  --tags [tag1, tag2] - run only tasks with either the tag tag1 or the tag tag2
  --skip-tags [tag3, tag4] - run all tasks except those with either the tag tag3 or the tag tag4
  --tags tagged - run only tasks with at least one tag
  --tags untagged - run only tasks with no tags

16. Ansible Config file?
 

-----------------------------------: DOCKER :-----------------------------------
1.  VM's                      Container
 - More time to create      - Less time to create
 - Dedicated OS             - Common OS
 - Specific bin/lib         - Common bin/lib
 - Memory wastage           - No memory wastage
 - Low Performance          - High performance
 - Height weight m/c        - Light weight m/c
 - Complex configuration    - Less configuration
 - Memory can't share       - memory can share

2. How to check multiple running containers CPU Usages:
 - # docker stats  --all
 - # docker stats <containerID>

3. Docker -itd vs -it?
 -itd: interactive terminal in detached mode, running in backend with /bin/bash. 
 -it: interactive terminal, container will create and we are entered into that container.

4. Docker Kill vs Stop?
 - STOP: Stop the running container and send the signal SIGTERM.	
 - Kill:  Kill one or more containers and send the signal SIGKILL.
  - Docker provides the kill command for sending signals to a container.

5. Docker Daemon?
 - A docker Deamon (Dokered ) listens for Docker API requests and manages Docker objects such as images, containers, volumes etc. 
 - Also it can communicate with other Docker Deamon to manage services.

6. Docker exec ?
 - Docker run using to to lunch the new container 
 - Docker exec runs a new command line in the running container.

7. How to get an Docker Image?
 - Docker pull <imagename>: pull an image from docker hub
 - Docker commit: convert a container into an image, docker commit  <container id> myimage
 - Docker build: build a script from docker file.

8. Docker Port Mapping: 
  - docker pull nginx
  - docker run -d --name <name> -p 8080:80 nginx:latest
 Docker Volume: Copy the data from local to nginx container: 
  - docker run --name website -v <desktop_path>:</usr/share/nginx/html> -d -p 8080:80 nginx:latest
  - docker exec -it <containername/containerID> bash

9. Docker File??
 - Create an image automatically using a build script.
Ex:
 -  Vi Dockerfile
 - FROM ubuntu
 - MAINTAINER xyz
 - RUN apt-get update
 - RUN echo “HI”
 - CMD [ “echo” “HELLO WORLD”]
 - #docker build –t myimage:1.0 .
 - #docker run -d --name <name> myimage:1.0

10. DOCKER ContainerIP?
 - 172.17.0.1: host
 - docker ispect <containerID>
 
11. DOCKER FILE: Single vs Multiple "RUN" commands ?
 - After building the image all layers stored under "Docker Cache", Suppose we later modify apt-get install by adding extra pkg
 - Docker sees the initial & modified instructions as identical & reuses the cache from previous steps.
 - As a result the apt-get is not executed bcz the build uses the cached version. 
 - So build can potentially get an outdated version of the curl / nginx pkg. 
 - Using RUN apt-get update && apt-get install -y nginx , ensures the Dockerfile installs latest pks. 
 - This technique is known as "cache busting"

12. How many Docker components are there?
 There are three docker components, they are: Docker Client, Docker Host and Docker Registry.
  1. Docker Client: 
    - This component performs “build” and “run” operations for the purpose of opening communication with the docker host.
  2. Docker Host: 
    - This component has the main docker daemon and hosts containers and their associated images. 
    - The daemon establishes a connection with the docker registry.
  3. Docker Registry: 
    - This component stores the docker images. There can be a public registry or a private one.
    - The most famous public registries are Docker Hub and Docker Cloud.

13. what happned to container logs if you stop/restarted the container ? logs deleted or what?
 - we won't be loosing any logs.
 - /var/logs/
 - if deleted the container then will loose it, or we can push it to splunk. 

14. If docker image 2.5gb, what will be cause of concern? how to tackle this?
 - building time will be longer.
 - downloaded error if we're trying to cownload from docker repo.
 - application will be bulkier.
Solutions:
 - Alpine images
 - remove package binaries after installing , never install those are not required.

--------: LINUX :-------------
1. Application port is running or not ?
 - command using 'netstat -lntp'
 - ex- tomcat 8080,nginx 80

2. Memory Check                                                       
 - "TOP" Able to see the CPU Usages 
 - "free -h" primary memory RAM Usages
 - "df -h" secondary memory HDD Usages
 - "du" directory memory check. 
 - uptime: Load Average

3. SOFT LINK VS HARD LINK & INODE IN LINUX:
 - An inode is a data structure that stores various information about a file in Linux,
   Containing information (the metadata) about a regular file and directory.
   Inode: Uniqe number for every file in Linux
 - Soft lInk: ln -s demo.txt Slink , only store the path, no data can store here , different inode value
 - Hard Link: ln demo.txt Hlink , contains data , replica of actual file, similar Inode value, if file is deleted can access the data from Hlink.

4. LINUX BOOTSTRAP PROCESS:
  BIOS: Basic Ip/op system , executes MBR
  MBR: Master boot record , executes GRUB
  GRUB: Grand Unified boot loader , executes kernel
  KERNEL: executes /sbin/init
  INIT: executes run level program
  RUNLEVEL:

5. Linux Run-lables.
 0 shutdown
 1 single user
 2 multiuser
 3 multiuser with n/wing
 4 notused
 5 GUI
 6 Reboot

6. What is " Bastian hosts / gateway server " ? what role do they play ?
 - It's a server with some user related configuration, like user can have certain permissions to access xyz server.
 - It's a gateway like certain users can access ec2 servers.
 - Used to manage access to internal or private network. 

7. If instance terminated ? how to debug the application ?
 - Terminated means unhealthy , factors ?
 - disk space full, high cpu utilisation, no memory left
 - Use top command -> cpu (application could be causing cpu utilisation, need to check with Developer like threading)
 - checking disk space -> /var/log, /tmp
 - free -mt -> swap memory

8. cron-job ?
 - to run the script in a particular interval. 

9. 

 =======================================================================================================
 what are the primary role you're doing in k8s?
