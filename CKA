------------------------: Exam Overview CKA : ---------------------
CKA : 50% , Cluster Installation , upgrade, Maintenance & Troubleshoot. 
CKAD : 50% , SYLLABUS IS SAME , Software / Application :Pod

1. Installer : Kubeadm --> (9 marks) - 100%
2. etcd backup & restore --> (6 marks) - 100% --> two cmds
 - etcdctl
 - 2nd link
 - --key & --cacert value would be given
3. deployment, release & rollout deployment --> (5/6 marks) - 100%

4. Volume, ConfigMaps, Secrets -->20 Marks
 - PC & PVC --> (8/9 marks) --> 100%

====================================================================
By default k8s doesn't support 
 - monitoring
 - logging
 - tracing
we need to add plugin's --> 3rd party

------------------:K8s Cluster:-------------------------------------------------------------
 - Control Plane -> 1 / 2
 - Worker Nodes -> 1 / 2
 - CRI (Container runtime)
 - CNI (container network interface), Overlay n/w
 - CSI (container storage interface)

=========================:Installer : Kubeadm:===============================================
Paste under userdata:

#!/bin/bash
swapoff -a
sudo apt-get update
sudo apt-get install -y docker.io/containerd jq wget curl  
sudo apt-get install -y apt-transport-https ca-certificates curl
sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
sudo usermod -aG docker ubuntu

------------------------its not mandatory but easily communicate---------
Rename hostname :--> It's for Ubuntu M/c
 cat /etc/hosts
to change the hostname:
 - hostnamectl set-hostname master
 - hostname master
 - vi /etc/hosts/
 - to get private ip
   22.43.5.1 Master
   3.4.22.34 Node1
---------------------------- :Creating a cluster with kubeadm: ---------------
In Master 
sudo kubeadm init --apiserver-advertise-address=<ip-address_privateip> | tee kubeadm-output.txt


How to get join command once again:
# kubeadm token create --print-join-command

In the exam :
Cmd to install CNI given to you. 


====================================:CNI:===========================================================================
- https://docs.tigera.io/calico/3.25/getting-started/kubernetes/self-managed-onprem/onpremises
- curl https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico-typha.yaml -o calico.yaml
kubectl apply -f calico.yaml

==============================:Exam --> Better to write Imperative way: ========================================

# kubectl api-resources | more -->how many resources we can manage/create.
 - false: can't be run in ns.
 - true: can be deployed within the namespace.
# kubectl api-resources | grep -i rs
# kubectl api-versions
# kubectl explain pod
# kubectl explain pod.metadata | more
# kubectl explain pod.spec | grep -i require
# kubectl get nodes -o wide --> runtime, internalip, os image, kernel version

------------------------------------------------------
# kubectl run <pod_name> --image nginx --port 80
# kubectl get pod -o wide --> ip & node in which it is running.
# kubectl describe pod <pod_namae> -o wide | more -->lables, status,ip, container info, host port numnber etc and "events"
 - from the events we can use for troubleshooting 

------------------ :dry-run=client -o yaml > 1.yaml: ---------------------------------------
IMP Command: ---> complete yml file. (--dry-run=client -o yaml > 1.yaml)
# kubectl run <pod_name> nginx --port 80 --dry-run=client -o yaml > 1.yaml
# kubectl apply -f 1.yaml 
# curl http://<POD_ip>

# kubectl create deployment abc --name nginx --dry-run=client -o yaml > 2.yaml

---------------------Basic Troubleshooting commands:-------------------------------
 kubectl logs <pod_name>
 kubectl logs  <pod_name> -f 
 kubectl logs <pod_name> -c <container_name>


=============================: Namespace :=========================================================================
1. default
2. kube-system 
3. kube-public
4. kube-node-lease

============================: Controllers: ========================================================================
1. ReplicaSet() --> desired count/replicas & actual count matches, any mismatch going to do self healing.
2. Deployment --> Gives us rollout & rollback
3. DeamonSet --> One pod per node.
4. statefulSet --> Maintain a sticky identity for each of their pods.

1. ReplicaSet:
 - self healing
 - scale out / scale In
 # kubectl get rs -o wide
 # kubectl get pods
 # kubectl get pod,rs -o wide
 # kubectl describe rs <name_rs>

2. StatefulSet: Stateful Applications.
 - Is we have stateful application like data bases, mysql,mongodb etc ... 
 - Maintain a sticky identity for each of their pods
 - with same identity. 
 when:
 - stable, unique n/w identifier. 
 - stable, persistent storage.
 - orered, automated rolling updates
 - ordered, graceful deployment and scaling.
 # kubectl get statefulset -o wide

3. DeamonSet:
 - One pod per node, monitoring, loggings etc, 
 - if we add new node, automatically added into that pod. 
 # kubectl get ds -n kube-system

4. Deployment: Stateless Application
 - deployment deploy rs
 - rs creates pod
 - Gives us rollout & rollback -->imp feature. 
 # kubectl create deployment <deployment_name> --image <image_name> --replicas 3 --dry-run=client -o yaml > 1.yaml
 # kubectl get deploy
 # kubectl get deploy,rs,pod -o wide
 # kubectl scale deployment d1 --replicas=5
 # kubectl set image deploy/d1 <image_name>=nginx:1.41.1
 # kubectl rollout undo deployment/d1
 or,
 # kubectl rollout undo deployment/d1 --to-revision=2

Ex: with the help of "labels & selector" pod is bind with deployment.
 apiVersion: apps/v1
 kind: Deployment
 metadata:
  labels:
   owner: nginx
 spec:
  replicas: 3
  selector:  
           -------> to filter out.
   matchLabels:      
    app: nginx

  template:           ------> Pod template spec.
   metadata:
    labels:
     app: nginx
   spec:
    containers:
    - name: nginx
      image: nginx:1.1
      ports:
       - containerPort: 80

Rollout deployment:

=============================: K8S Services: =====================================================================
- Enabling network. 
1. ClusterIP ( Port - 443 / TCP ) --> default
2. NodePort ( 30,000 - 32,767) / TCP --> Stable IP Address.
3. LoadBalancer --> more of network load balancer. ( Layer 4)
    - Requires public IP / External IP
    - will work when deploy to EKS, AKS, GKE etc. 

4. Ingress:  --> Application Load balancer (layer 7)
 - http/https
 - path based routing()
 - host based routing
 - content 
---------------------------------------------
1. ClusterIP:
 - Within the cluster
 - Inside the cluster can access the application. 
 - By using labels & selector we can bind the pods using clusterIP
Ex:
apiVesrion: v1         apiVersion: v1
kind: Service          kind: pod
metatdata:             metadata:
 name: ingress          name:
spec:                   "labels: nginx-backend"
 type: clusterIP       spec:
 ports:                 containers: 
  - name: http          - image:
    port: 80              name:
    targetPort: 80        ports:
    protocol: TCP          - containerPort: 80 -->targetport
 selector:
  "app: nginx- backend"

2. NodePort:
 - Outside the cluster. 
 - opens a specific port on all the nodes in the cluster. 
 - A ClusterIP Service, to which the NodePort Service routes, is automatically created. 
 - You'll be able to contact the NodePort Service, from outside the cluster, by requesting <NodeIP>:<NodePort>

Ex:
apiVesrion: v1         
kind: Service          
metatdata:             
 name: ingress          
spec:                   
 type: NodePort       
 ports:                 
  - name: http          
    port: 80              
    targetPort: 80  --> app port
    nodePort: 30080 --> node port       
    protocol: TCP          
 selector:
  "app: nginx- backend"

# kubectl get svc
# kubectl get svc -0 wide
# kubectl describe svc <name_of_svc> --> we can see endpoints.
# kubectl get nodes -o wide
# <node_IP>:<node_port>
# kubectl run abc --image nginx -l <selector>
# kubectl expose --help  ---> we can use in exam
# kubectl expose deploy/pod d1 --port=80 --targetport=80 --type NodePort --name <service_name> --dry-run=client -o yaml

Limitations:
 - Never use in production or practical world
 - 

========================= "K8S VOLUME" ======================================================================
Types:
 - cephs
 - configMap
 - emptydir
 - persistentvolume
 - persistentVolumeClaim
 - local
 - nfs
 - hostpath
 - secret
 - fc (fibre channel)

Node local memory (emptydir & hostPath) --> exam point not in real world like prod. 
Cloud volumes
File sharing volumes (nfs, network file system)
Distributed file systems (cephFS & glusterFS)
Special volume types (persistentVolumeClaim, secret, configMap)

1. emptyDir:
 - emptyDir volume is first created when a pod is assigned to a node. 
 - when a pod dies / removed from a node for any reason, the data in the emptyDIr is deleted forever.
 - mainly used to store cache / temporary data to be processed. 
Note:
 - volumeMounts "name" & volumes "name" should match.

2. hostPath:
 - if pod dies, data will be there.
 - volume mounts a file/dir from the host node filesystem into our pod.
 - when node becomes unstable, the pods might fail to access the hostPath dir & eventually gets terminated.

Mountvolume.setup failed --> we need to create a dir
 - mkdir /data

3. PV & PVC: --> Real world, --> infra team will provide the storage. 
 - Cluster-wide resource used to store the data beyond the lifetime of a pod.
 - in order to use pv , we need to first claim it using pvc.
 - size, speed, read write properties etc
--------------
 Storage Admin
  - PV1 3gb
  - PV2 2gb --> bound
  - PV3 500mb 

developer --> pod --> 2gb asked --> PV2 as volume

